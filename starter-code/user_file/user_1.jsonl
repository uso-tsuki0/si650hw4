{"docid": 18933234, "title": "Emacs", "text": "Emacs , originally named EMACS (an acronym for \"Editor MACroS\"), is a family of text editors that are characterized by their extensibility. The manual for the most widely used variant, GNU Emacs, describes it as \"the extensible, customizable, self-documenting, real-time display editor\". Development of the first Emacs began in the mid-1970s, and work on its direct descendant, GNU Emacs, continues actively; the latest version is 28.2, released in September 2022.\nEmacs has over 10,000 built-in commands and its user interface allows the user to combine these commands into macros to automate work. Implementations of Emacs typically feature a dialect of the Lisp programming language, allowing users and developers to write new commands and applications for the editor. Extensions have been written to, among other things, manage files, remote access, e-mail, outlines, multimedia, Git integration, and RSS feeds, as well as implementations of \"ELIZA\", \"Pong\", \"Conway's Life\", \"Snake\", \"Dunnet\", and \"Tetris\".\nThe original EMACS was written in 1976 by David A. Moon and Guy L. Steele Jr. as a set of Editor MACroS for the TECO editor. It was inspired by the ideas of the TECO-macro editors TECMAC and TMACS.\nThe most popular, and most ported, version of Emacs is GNU Emacs, which was created by Richard Stallman for the GNU Project. XEmacs is a variant that branched from GNU Emacs in 1991. GNU Emacs and XEmacs use similar Lisp dialects and are, for the most part, compatible with each other. XEmacs development is inactive.\nEmacs is, along with vi, one of the two main contenders in the traditional editor wars of Unix culture. Emacs is among the oldest free and open source projects still under development.\nHistory.\nEmacs development began during the 1970s at the MIT AI Lab, whose PDP-6 and PDP-10 computers used the Incompatible Timesharing System (ITS) operating system that featured a default line editor known as Tape Editor and Corrector (TECO). Unlike most modern text editors, TECO used separate modes in which the user would either add text, edit existing text, or display the document. One could not place characters directly into a document by typing them into TECO, but would instead enter a character ('i') in the TECO command language telling it to switch to input mode, enter the required characters, during which time the edited text was not displayed on the screen, and finally enter a character () to switch the editor back to command mode. (A similar technique was used to allow overtyping.) This behavior is similar to that of the program ed.\nBy the 1970s, TECO was already an old program, initially released in 1962. Richard Stallman visited the Stanford AI Lab in 1976 and saw the lab's \"E\" editor, written by Fred Wright. He was impressed by the editor's intuitive WYSIWYG (What You See Is What You Get) behavior, which has since become the default behavior of most modern text editors. He returned to MIT where Carl Mikkelsen, a hacker at the AI Lab, had added to TECO a combined display/editing mode called \"Control-R\" that allowed the screen display to be updated each time the user entered a keystroke. Stallman reimplemented this mode to run efficiently and then added a macro feature to the TECO display-editing mode that allowed the user to redefine any keystroke to run a TECO program.\nE had another feature that TECO lacked: random-access editing. TECO was a page-sequential editor that was designed for editing paper tape on the PDP-1 at a time when computer memory was generally small due to cost, and it was a feature of TECO that allowed editing on only one page at a time sequentially in the order of the pages in the file. Instead of adopting E's approach of structuring the file for page-random access on disk, Stallman modified TECO to handle large buffers more efficiently and changed its file-management method to read, edit, and write the entire file as a single buffer. Almost all modern editors use this approach.\nThe new version of TECO quickly became popular at the AI Lab and soon accumulated a large collection of custom macros whose names often ended in \"MAC\" or \"MACS\", which stood for \"macro\". Two years later, Guy Steele took on the project of unifying the diverse macros into a single set. Steele and Stallman's finished implementation included facilities for extending and documenting the new macro set. The resulting system was called EMACS, which stood for \"Editing MACroS\" or, alternatively, \"E with MACroS\". Stallman picked the name Emacs \"because  was not in use as an abbreviation on ITS at the time.\" An apocryphal hacker koan alleges that the program was named after \"Emack & Bolio's\", a popular Boston ice cream store. The first operational EMACS system existed in late 1976.\nStallman saw a problem in too much customization and \"de facto\" forking and set certain conditions for usage. He later wrote:\nThe original Emacs, like TECO, ran only on the PDP-10 running ITS. Its behavior was sufficiently different from that of TECO that it could be considered a text editor in its own right, and it quickly became the standard editing program on ITS. Mike McMahon ported Emacs from ITS to the TENEX and TOPS-20 operating systems. Other contributors to early versions of Emacs include Kent Pitman, Earl Killian, and Eugene Ciccarelli. By 1979, Emacs was the main editor used in MIT's AI lab and its Laboratory for Computer Science.\nImplementations.\nEarly implementations.\nIn the following years, programmers wrote a variety of Emacs-like editors for other computer systems. These included EINE (\"EINE Is Not EMACS\") and ZWEI (\"ZWEI Was EINE Initially\"), which were written for the Lisp machine by Mike McMahon and Daniel Weinreb, and Sine (\"Sine Is Not Eine\"), which was written by Owen Theodore Anderson. Weinreb's EINE was the first Emacs written in Lisp. In 1978, Bernard Greenberg wrote Multics Emacs almost entirely in Multics Lisp at Honeywell's Cambridge Information Systems Lab. Multics Emacs was later maintained by Richard Soley, who went on to develop the NILE Emacs-like editor for the NIL Project, and by Barry Margolin. Many versions of Emacs, including GNU Emacs, would later adopt Lisp as an extension language.\nJames Gosling, who would later invent NeWS and the Java programming language, wrote Gosling Emacs in 1981. The first Emacs-like editor to run on Unix, Gosling Emacs was written in C and used Mocklisp, a language with Lisp-like syntax, as an extension language.\nEarly Ads for Computer Corporation of America's \"CCA EMACS\" (Steve Zimmerman). appeared in 1984. 1985 comparisons to GNU Emacs, when it came out, mentioned free vs. $2,400.\nGNU Emacs.\nRichard Stallman began work on GNU Emacs in 1984 to produce a free software alternative to the proprietary Gosling Emacs. GNU Emacs was initially based on Gosling Emacs, but Stallman's replacement of its Mocklisp interpreter with a true Lisp interpreter required that nearly all of its code be rewritten. This became the first program released by the nascent GNU Project. GNU Emacs is written in C and provides Emacs Lisp, also implemented in C, as an extension language. Version 13, the first public release, was made on March 20, 1985. The first widely distributed version of GNU Emacs was version 15.34, released later in 1985. Early versions of GNU Emacs were numbered as \"1.x.x\", with the initial digit denoting the version of the C core. The \"1\" was dropped after version 1.12, as it was thought that the major number would never change, and thus the numbering skipped from \"1\" to \"13\". In September 2014, it was announced on the GNU emacs-devel mailing list that GNU Emacs would adopt a rapid release strategy and version numbers would increment more quickly in the future.\nGNU Emacs offered more features than Gosling Emacs, in particular a full-featured Lisp as its extension language, and soon replaced Gosling Emacs as the \"de facto\" Unix Emacs editor. Markus Hess exploited a security flaw in GNU Emacs' email subsystem in his 1986 cracking spree in which he gained superuser access to Unix computers.\nMost of GNU Emacs functionality is implemented through a scripting language called Emacs Lisp. Because about 70% of GNU Emacs is written in the Emacs Lisp extension language, one only needs to port the C core which implements the Emacs Lisp interpreter. This makes porting Emacs to a new platform considerably less difficult than porting an equivalent project consisting of native code only.\nGNU Emacs development was relatively closed until 1999 and was used as an example of the \"Cathedral\" development style in \"The Cathedral and the Bazaar\". The project has since adopted a public development mailing list and anonymous CVS access. Development took place in a single CVS trunk until 2008 and was then switched to the Bazaar DVCS. On November 11, 2014, development was moved to Git.\nRichard Stallman has remained the principal maintainer of GNU Emacs, but he has stepped back from the role at times. Stefan Monnier and Chong Yidong were maintainers from 2008 to 2015. John Wiegley was named maintainer in 2015 after a meeting with Stallman at MIT. As of early 2014, GNU Emacs has had 579 individual committers throughout its history.\nXEmacs.\nLucid Emacs, based on an early alpha version of GNU Emacs 19, was developed beginning in 1991 by Jamie Zawinski and others at Lucid Inc. One of the best-known early forks in free software development occurred when the codebases of the two Emacs versions diverged and the separate development teams ceased efforts to merge them back into a single program. Lucid Emacs has since been renamed XEmacs. Its development is currently inactive, with the most recent stable version 21.4.22 released in January 2009 (while a beta was released in 2013), while GNU Emacs has implemented many formerly XEmacs-only features.\nOther forks of GNU Emacs.\nOther notable forks include:\nVarious Emacs editors.\nIn the past, projects aimed at producing small versions of Emacs proliferated. GNU Emacs was initially targeted at computers with a 32-bit flat address space and at least 1\u00a0MiB of RAM. Such computers were high end workstations and minicomputers in the 1980s, and this left a need for smaller reimplementations that would run on common personal computer hardware. Today's computers have more than enough power and capacity to eliminate these restrictions, but small clones have more recently been designed to fit on software installation disks or for use on less capable hardware.\nOther projects aim to implement Emacs in a different dialect of Lisp or a different programming language altogether. Although not all are still actively maintained, these clones include:\nFeatures.\nEmacs is primarily a text editor and is designed for manipulating pieces of text, although it is capable of formatting and printing documents like a word processor by interfacing with external programs such as LaTeX, Ghostscript or a web browser. Emacs provides commands to manipulate and differentially display semantic units of text such as words, sentences, paragraphs and source code constructs such as functions. It also features \"keyboard macros\" for performing user-defined batches of editing commands.\nGNU Emacs is a \"real-time display\" editor, as its edits are displayed onscreen as they occur. This is standard behavior for modern text editors but EMACS was among the earliest to implement this. The alternative is having to issue a distinct command to display text, (e.g. before or after modifying it). This was common in earlier (or merely simpler) line and context editors, such as QED (BTS, CTSS, Multics), ed (Unix), ED (CP/M), and Edlin (DOS).\nGeneral architecture.\nAlmost all of the functionality in Emacs, including basic editing operations such as the insertion of characters into a file, is achieved through functions written in a dialect of the Lisp programming language. The dialect used in GNU Emacs is known as Emacs Lisp (Elisp), and was developed expressly to port Emacs to GNU and Unix. The Emacs Lisp layer sits atop a stable core of basic services and platform abstraction written in the C programming language, which enables GNU Emacs to be ported to a wide variety of operating systems and architectures without modifying the implementation semantics of the Lisp system where most of the editor lives. In this Lisp environment, variables and functions can be modified with no need to rebuild or restart Emacs, with even newly redefined versions of core editor features being asynchronously compiled and loaded into the live environment to replace existing definitions. Modern GNU Emacs features both bytecode and native code compilation for Emacs Lisp.\nAll configuration is stored in variables, classes, and data structures, and changed by simply updating these live. The use of a Lisp dialect in this case is a key advantage, as Lisp syntax consists of so-called symbolic expressions (or sexprs), which can act as both evaluatable code expressions and as a data serialisation format akin to, but simpler and more general than, well known ones such as XML, JSON, and YAML. In this way there is little difference in practice between customising existing features and writing new ones, both of which are accomplished in the same basic way. This is operatively different from most modern extensible editors, for instance such as VS Code, in which separate languages are used to implement the interface and functions of the editor and encode its user-defined configuration and options. The goal of Emacs' open design is to transparently expose Emacs' internals to the Emacs user during normal use in the same way that they would be exposed to the Emacs developer working on the git tree, and to collapse as much as possible of the distinction between using Emacs and programming Emacs, while still providing a stable, practical, and responsive editing environment for novice users.\nInteractive data.\nThe main text editing data structure is the \"buffer\", a memory region containing data (usually text) with associated attributes. The most important of these are:\n\"Modes\", in particular, are an important concept in Emacs, providing a mechanism to disaggregate Emacs' functionality into sets of behaviours and keybinds relevant to specific buffers' data. \"Major modes\" provide a general package of functions and commands relevant to a buffer's data and the way users might be interacting with it (e.g. editing source code in a specific language, editing hex, viewing the filesystem, interacting with git, etc.), and \"minor modes\" define subsidiary collections of functionality applicable across many major modes (such as codice_1). Minor modes can be toggled on or off both locally to each buffer as well as globally across all buffers, while major modes can only be toggled per-buffer. Any other data relevant to a buffer but not bundled into a mode can be handled by simply focussing that buffer and live modifying the relevant data directly.\nAny interaction with the editor (like key presses or clicking a mouse button) is realized by evaluating Emacs Lisp code, typically a \"command\", which is a function explicitly designed for interactive use. Keys can be arbitrarily redefined and commands can also be accessed by name; some commands evaluate arbitrary Emacs Lisp code provided by the user in various ways (e.g. a family of codice_2 functions, operating on the codice_3, codice_4, or individual codice_5). Even the simplest user inputs (such a printable characters) are effectuated as Emacs Lisp functions, such as the codice_6 , bound by default to most keyboard keys in a typical text editing buffer, which parameterises itself with the locale-defined character associated with the key used to call it. \nFor example, pressing the key in a buffer that accepts text input evaluates the code , which inserts one copy of the character constant codice_7 \"at point\". The codice_8, in this case, is determined by what Emacs terms the \"universal argument\": all Emacs command code accepts a numeric value which, in its simplest usage, indicates repetition of an action, but in more complex cases (where repetition doesn't make sense) can yield other behaviours. These arguments may be supplied via command prefices, such as , or more compactly , which expands to . When no prefix is supplied, the universal argument is codice_8: every command implicitly runs once, but may be called multiply, or in a different way, when supplied with such a prefix. Such arguments may also be non-positive where it makes sense for them to be so - it is up to the function accepting the argument to determine, according to its own semantics, what a given number means to it. One common usage is for functions to perform actions in reverse simply by checking the sign of the univeral argument, such as a sort command which sorts in obverse by default and in reverse when called with a negative argument, using the absolute value of its argument as the sorting key (e.g. codice_10 sorting in reverse by column index (or delimiter) 7), or undo/redo, which are simply negatives of each other (traversing forward and backward through a recursive history of diffs by some number of steps at a time).\nCommand language.\nBecause of its relatively large vocabulary of commands, Emacs features a long-established \"command language\", to concisely express the keystrokes necessary to perform an action. This command language recognises the following shift and modifier keys: , , , , , and . Not all of these may be present on an IBM-style keyboard, though they can usually be configured as desired. These are represented in command language as the respective prefices: codice_11, codice_12, codice_13, codice_14, codice_15, and codice_16. Keys whose names are only printable with more than one character are enclosed in angle brackets. Thus, a keyboard shortcut such as (check dependent formulas and calculate all cells in all open workbooks in Excel) would be rendered in Emacs command language as codice_17, while an Emacs command like (incremental file search by filename-matching regexp), would be expressed as codice_18. Command language is also used to express the actions needed to invoke commands with no assigned shortcut: for example, the command codice_19 (which initialises a buffer in memory for temporary text storage and manipulation), when invoked by the user, will be reported back as codice_20, with Emacs scanning the namespace of contextually available commands to return the shortest sequence of keystrokes which uniquely lexicate it.\nDynamic display.\nBecause Emacs predates modern standard terminology for graphical user interfaces, it uses somewhat divergent names for familiar interface elements. Buffers, the data that Emacs users interact with, are displayed to the user inside \"windows\", which are tiled portions of the terminal screen or the GUI window, which Emacs refers to as \"frames\"; in modern terminology, an Emacs \"frame\" would be a window and an Emacs \"window\" would be a split. Depending on configuration, windows can include their own scroll bars, line numbers, sometimes a 'header line' typically to ease navigation, and a \"mode line\" at the bottom (usually displaying buffer name, the active modes and point position of the buffer among others). The bottom of every frame is used for output messages (then called 'echo area') and text input for commands (then called 'minibuffer').\nIn general, Emacs display elements (windows, frames, etc.) do not belong to any specific data or process. Buffers are not associated with windows, and multiple windows can be opened onto the same buffer, for example to track different parts of a long text side-by-side without scrolling back and forth, and multiple buffers can share the same text, for example to take advantage of different major modes in a mixed-language file. Similarly, Emacs instances are not associated with particular frames, and multiple frames can be opened displaying a single running Emacs process, e.g. a frame per screen in a multi-monitor setup, or a terminal frame connected via ssh from a remote system and a graphical frame displaying the same Emacs process via the local system's monitor. Just as buffers don't require windows, running Emacs processes do not require any frames, and one common usage pattern is to deploy Emacs as an \"editing server\": running it as a headless daemon and connecting to it via a frame-spawning client. This server can then be made available in any situation where an editor is required, simply by declaring the client program to be the user's codice_21 or codice_22 variable. Such a server continues to run in the background, managing any child processes, accumulating stdin from open pipes, ports, or fifos, performing periodic or pre-programmed actions, remembering buffer undo history, saved text snippets, command history, and other user state between editing sessions. In this mode of operation, Emacs overlaps the functionality of programs like screen and tmux.\nBecause of this separation of display concerns from editing functionality, Emacs can display roughly similarly on any device more complex than a dumb terminal, including providing typical graphical WIMP elements on sufficiently featureful text terminals - though graphical frames are the preferred mode of display, providing a strict superset of the features of text terminal frames.\nSelf-documenting.\nThe first Emacs contained a \"help\" library that included documentation for every command, variable and internal function. Because of this, Emacs proponents described the software as \"self-documenting\" in that it presents the user with information on its normal features and its current state. Each function includes a documentation string that is displayed to the user on request, a practice that subsequently spread to programming languages including Lisp, Java, Perl, and Python. This help system can take users to the actual code for each function, whether from a built-in library or an added third-party library.\nEmacs also has a built-in tutorial. Emacs displays instructions for performing simple editing commands and invoking the tutorial when it is launched with no file to edit. The tutorial is by Stuart Cracraft and Richard Stallman.\nCulture.\nChurch of Emacs.\nThe \"Church of Emacs\", formed by Richard Stallman, is a parody religion created for Emacs users. While it refers to vi as the \"editor of the beast\" (vi-vi-vi being 6-6-6 in Roman numerals), it does not oppose the use of vi; rather, it calls it proprietary software anathema. (\"Using a free version of vi is not a sin but a penance.\") The Church of Emacs has its own newsgroup, , that has posts purporting to support this parody religion. Supporters of vi have created an opposing \"Cult of vi\".\nStallman has jokingly referred to himself as \"St I\u200aGNU\u200acius\", a saint in the Church of Emacs.\nEmacs pinky.\nThere is folklore attributing a repetitive strain injury colloquially called \"Emacs pinky\" to Emacs' strong dependence on modifier keys, although there have not been any studies done to show Emacs causes more such problems than other keyboard-heavy computer programs.\nUsers have addressed this through various approaches. Some users recommend simply using the two Control keys on typical PC keyboards like Shift keys while touch typing to avoid overly straining the left pinky, a proper use of the keyboard will reduce the RSI. Software-side methods include:\nHardware solutions include special keyboards such as Kinesis's Contoured Keyboard, which places the modifier keys where they can easily be operated by the thumb, or the Microsoft Natural keyboard, whose large modifier keys are placed symmetrically on both sides of the keyboard and can be pressed with the palm of the hand. Foot pedals can also be used.\nThe \"Emacs pinky\" is a relatively recent development. The Space-cadet keyboard on which Emacs was developed had oversized Control keys that were adjacent to the space bar and were easy to reach with the thumb.\nTerminology.\nThe word \"emacs\" is sometimes pluralized as \"emacsen\", by phonetic analogy with boxen and VAXen, referring to different varieties of Emacs.", "categories": ["Category:1970s in computing", "Category:1976 software", "Category:All articles lacking reliable references", "Category:All articles with unsourced statements", "Category:Articles lacking reliable references from January 2021", "Category:Articles lacking reliable references from July 2019", "Category:Articles with short description", "Category:Articles with unsourced statements from January 2011", "Category:Articles with unsourced statements from November 2022", "Category:Commons category link from Wikidata"]}
{"docid": 273993, "title": "Stack (abstract data type)", "text": "In computer science, a stack is an abstract data type that serves as a collection of elements, with two main operations:\nAdditionally, a peek operation can, without modifying the stack, return the value of the last element added. Calling this structure a \"stack\" is by analogy to a set of physical items stacked one atop another, such as a stack of plates. \nThe order in which an element added to or removed from a stack is described as last in, first out, referred to by the acronym LIFO. As with a stack of physical objects, this structure makes it easy to take an item off the top of the stack, but accessing a datum deeper in the stack may require taking off multiple other items first.\nConsidered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the \"top\" of the stack. This data structure makes it possible to implement a stack as a singly linked list and as a pointer to the top element. A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept another element, the stack is in a state of stack overflow.\nA stack is needed to implement depth-first search.\nHistory.\nStacks entered the computer science literature in 1946, when Alan M. Turing used the terms \"bury\" and \"unbury\" as a means of calling and returning from subroutines. Subroutines had already been implemented in Konrad Zuse's Z4 in 1945.\nKlaus Samelson and Friedrich L. Bauer of Technical University Munich proposed the idea of a stack in 1955 and filed a patent in 1957. In March 1988, by which time Samelson was deceased, Bauer received the IEEE Computer Pioneer Award for the invention of the stack principle. Similar concepts were developed, independently, by Charles Leonard Hamblin in the first half of 1954 and by in 1958.\nStacks are often described using the analogy of a spring-loaded stack of plates in a cafeteria. Clean plates are placed on top of the stack, pushing down any already there. When a plate is removed from the stack, the one below it pops up to become the new top plate.\nNon-essential operations.\nIn many implementations, a stack has more operations than the essential \"push\" and \"pop\" operations. An example of a non-essential operation is \"top of stack\", or \"peek\", which observes the top element without removing it from the stack. This could be done with a \"pop\" followed by a \"push\" to return the same data to the stack, so it is not considered an essential operation. If the stack is empty, an underflow condition will occur upon execution of either the \"stack top\" or \"pop\" operations. Additionally, many implementations provide a check if the stack is empty and one that returns its size.\nSoftware stacks.\nImplementation.\nA stack can be easily implemented either through an array or a linked list, as stacks are just special cases of lists. What identifies the data structure as a stack, in either case, is not the implementation but the interface: the user is only allowed to pop or push items onto the array or linked list, with few other helper operations. The following will demonstrate both implementations, using pseudocode.\nArray.\nAn array can be used to implement a (bounded) stack, as follows. The first element, usually at the zero offset, is the bottom, resulting in codice_1 being the first element pushed onto the stack and the last element popped off. The program must keep track of the size (length) of the stack, using a variable \"top\" that records the number of items pushed so far, therefore pointing to the place in the array where the next element is to be inserted (assuming a zero-based index convention). Thus, the stack itself can be effectively implemented as a three-element structure:\n structure stack:\n maxsize : integer\n top : integer\n items : array of item\n procedure initialize(stk : stack, size : integer):\n stk.items \u2190 new array of \"size\" items, initially empty\n stk.maxsize \u2190 size\n stk.top \u2190 0\nThe \"push\" operation adds an element and increments the \"top\" index, after checking for overflow:\n procedure push(stk : stack, x : item):\n if stk.top = stk.maxsize:\n report overflow error\n else:\n stk.items[stk.top] \u2190 x\n stk.top \u2190 stk.top + 1\nSimilarly, \"pop\" decrements the \"top\" index after checking for underflow, and returns the item that was previously the top one:\n procedure pop(stk : stack):\n if stk.top = 0:\n report underflow error\n else:\n stk.top \u2190 stk.top \u2212 1\n r \u2190 stk.items[stk.top]\n return r\nUsing a dynamic array, it is possible to implement a stack that can grow or shrink as much as needed. The size of the stack is simply the size of the dynamic array, which is a very efficient implementation of a stack since adding items to or removing items from the end of a dynamic array requires amortized O(1) time.\nLinked list.\nAnother option for implementing stacks is to use a singly linked list. A stack is then a pointer to the \"head\" of the list, with perhaps a counter to keep track of the size of the list:\n structure frame:\n data : item\n next : frame or nil\n structure stack:\n head : frame or nil\n size : integer\n procedure initialize(stk : stack):\n stk.head \u2190 nil\n stk.size \u2190 0\nPushing and popping items happens at the head of the list; overflow is not possible in this implementation (unless memory is exhausted):\n procedure push(stk : stack, x : item):\n newhead \u2190 new frame\n newhead.data \u2190 x\n newhead.next \u2190 stk.head\n stk.head \u2190 newhead\n stk.size \u2190 stk.size + 1\n procedure pop(stk : stack):\n if stk.head = nil:\n report underflow error\n r \u2190 stk.head.data\n stk.head \u2190 stk.head.next\n stk.size \u2190 stk.size - 1\n return r\nStacks and programming languages.\nSome languages, such as Perl, LISP, JavaScript and Python, make the stack operations push and pop available on their standard list/array types. Some languages, notably those in the Forth family (including PostScript), are designed around language-defined stacks that are directly visible to and manipulated by the programmer.\nThe following is an example of manipulating a stack in Common Lisp (\" is the Lisp interpreter's prompt; lines not starting with \" are the interpreter's responses to expressions):\n> (setf stack (list 'a 'b 'c)) ;; set the variable \"stack\"\n> (pop stack) ;; get top (leftmost) element, should modify the stack\nA\n> stack ;; check the value of stack\n> (push 'new stack) ;; push a new top onto the stack\nSeveral of the C++ Standard Library container types have and operations with LIFO semantics; additionally, the template class adapts existing containers to provide a restricted API with only push/pop operations. PHP has an SplStack class. Java's library contains a class that is a specialization of . Following is an example program in Java language, using that class.\nimport java.util.Stack;\nclass StackDemo {\n public static void main(String[]args) {\n Stack stack = new Stack();\n stack.push(\"A\"); // Insert \"A\" in the stack\n stack.push(\"B\"); // Insert \"B\" in the stack\n stack.push(\"C\"); // Insert \"C\" in the stack\n stack.push(\"D\"); // Insert \"D\" in the stack\n System.out.println(stack.peek()); // Prints the top of the stack (\"D\")\n stack.pop(); // removing the top (\"D\")\n stack.pop(); // removing the next top (\"C\")\nHardware stack.\nA common use of stacks at the architecture level is as a means of allocating and accessing memory.\nBasic architecture of a stack.\nA typical stack is an area of computer memory with a fixed origin and a variable size. Initially the size of the stack is zero. A \"stack pointer,\" usually in the form of a hardware register, points to the most recently referenced location on the stack; when the stack has a size of zero, the stack pointer points to the origin of the stack.\nThe two operations applicable to all stacks are:\nThere are many variations on the basic principle of stack operations. Every stack has a fixed location, in memory, at which it begins. As data items are added to the stack, the stack pointer is displaced to indicate the current extent of the stack, which expands away from the origin.\nStack pointers may point to the origin of a stack or to a limited range of addresses either above or below the origin (depending on the direction in which the stack grows); however, the stack pointer cannot cross the origin of the stack. In other words, if the origin of the stack is at address 1000 and the stack grows downwards (towards addresses 999, 998, and so on), the stack pointer must never be incremented beyond 1000 (to 1001, 1002, etc.). If a pop operation on the stack causes the stack pointer to move past the origin of the stack, a \"stack underflow\" occurs. If a push operation causes the stack pointer to increment or decrement beyond the maximum extent of the stack, a \"stack overflow\" occurs.\nSome environments that rely heavily on stacks may provide additional operations, for example:\nStacks are often visualized growing from the bottom up (like real-world stacks). They may also be visualized growing from left to right, so that \"topmost\" becomes \"rightmost\", or even growing from top to bottom. The important feature is that the bottom of the stack is in a fixed position. The illustration in this section is an example of a top-to-bottom growth visualization: the top (28) is the stack \"bottom\", since the stack \"top\" (9) is where items are pushed or popped from.\nA \"right rotate\" will move the first element to the third position, the second to the first and the third to the second. Here are two equivalent visualizations of this process:\n apple banana\n banana ===right rotate==> cucumber\n cucumber apple\n cucumber apple\n banana ===left rotate==> cucumber\n apple banana\nA stack is usually represented in computers by a block of memory cells, with the \"bottom\" at a fixed location, and the stack pointer holding the address of the current \"top\" cell in the stack. The top and bottom terminology are used irrespective of whether the stack actually grows towards lower memory addresses or towards higher memory addresses.\nPushing an item on to the stack adjusts the stack pointer by the size of the item (either decrementing or incrementing, depending on the direction in which the stack grows in memory), pointing it to the next cell, and copies the new top item to the stack area. Depending again on the exact implementation, at the end of a push operation, the stack pointer may point to the next unused location in the stack, or it may point to the topmost item in the stack. If the stack points to the current topmost item, the stack pointer will be updated before a new item is pushed onto the stack; if it points to the next available location in the stack, it will be updated \"after\" the new item is pushed onto the stack.\nPopping the stack is simply the inverse of pushing. The topmost item in the stack is removed and the stack pointer is updated, in the opposite order of that used in the push operation.\nStack in main memory.\nMany CISC-type CPU designs, including the x86, Z80 and 6502, have a dedicated register for use as the call stack stack pointer with dedicated call, return, push, and pop instructions that implicitly update the dedicated register, thus increasing code density. Some CISC processors, like the PDP-11 and the 68000, also have special addressing modes for implementation of stacks, typically with a semi-dedicated stack pointer as well (such as A7 in the 68000). In contrast, most RISC CPU designs do not have dedicated stack instructions and therefore most, if not all, registers may be used as stack pointers as needed.\nStack in registers or dedicated memory.\nSome machines use a stack for arithmetic and logical operations; operands are pushed onto the stack, and arithmetic and logical operations act on the top one or more items on the stack, popping them off the stack and pushing the result onto the stack. Machines that function in this fashion are called stack machines.\nA number of mainframes and minicomputers were stack machines, the most famous being the Burroughs large systems. Other examples include the CISC HP 3000 machines and the CISC machines from Tandem Computers.\nThe x87 floating point architecture is an example of a set of registers organised as a stack where direct access to individual registers (relative to the current top) is also possible.\nHaving the top-of-stack as an implicit argument allows for a small machine code footprint with a good usage of bus bandwidth and code caches, but it also prevents some types of optimizations possible on processors permitting random access to the register file for all (two or three) operands. A stack structure also makes superscalar implementations with register renaming (for speculative execution) somewhat more complex to implement, although it is still feasible, as exemplified by modern x87 implementations.\nSun SPARC, AMD Am29000, and Intel i960 are all examples of architectures using register windows within a register-stack as another strategy to avoid the use of slow main memory for function arguments and return values.\nThere are also a number of small microprocessors that implements a stack directly in hardware and some microcontrollers have a fixed-depth stack that is not directly accessible. Examples are the PIC microcontrollers, the Computer Cowboys MuP21, the Harris RTX line, and the Novix NC4016. Many stack-based microprocessors were used to implement the programming language Forth at the microcode level.\nApplications of stacks.\nExpression evaluation and syntax parsing.\nCalculators employing reverse Polish notation use a stack structure to hold values. Expressions can be represented in prefix, postfix or infix notations and conversion from one form to another may be accomplished using a stack. Many compilers use a stack for parsing the syntax of expressions, program blocks etc. before translating into low-level code. Most programming languages are context-free languages, allowing them to be parsed with stack-based machines.\nBacktracking.\nAnother important application of stacks is backtracking. Consider a simple example of finding the correct path in a maze. There are a series of points, from the starting point to the destination. We start from one point. To reach the final destination, there are several paths. Suppose we choose a random path. After following a certain path, we realise that the path we have chosen is wrong. So we need to find a way by which we can return to the beginning of that path. This can be done with the use of stacks. With the help of stacks, we remember the point where we have reached. This is done by pushing that point into the stack. In case we end up on the wrong path, we can pop the last point from the stack and thus return to the last point and continue our quest to find the right path. This is called backtracking.\nThe prototypical example of a backtracking algorithm is depth-first search, which finds all vertices of a graph that can be reached from a specified starting vertex. Other applications of backtracking involve searching through spaces that represent potential solutions to an optimization problem. Branch and bound is a technique for performing such backtracking searches without exhaustively searching all of the potential solutions in such a space.\nCompile-time memory management.\nA number of programming languages are stack-oriented, meaning they define most basic operations (adding two numbers, printing a character) as taking their arguments from the stack, and placing any return values back on the stack. For example, PostScript has a return stack and an operand stack, and also has a graphics state stack and a dictionary stack. Many virtual machines are also stack-oriented, including the p-code machine and the Java Virtual Machine.\nAlmost all calling conventionsthe ways in which subroutines receive their parameters and return resultsuse a special stack (the \"call stack\") to hold information about procedure/function calling and nesting in order to switch to the context of the called function and restore to the caller function when the calling finishes. The functions follow a runtime protocol between caller and callee to save arguments and return value on the stack. Stacks are an important way of supporting nested or recursive function calls. This type of stack is used implicitly by the compiler to support CALL and RETURN statements (or their equivalents) and is not manipulated directly by the programmer.\nSome programming languages use the stack to store data that is local to a procedure. Space for local data items is allocated from the stack when the procedure is entered, and is deallocated when the procedure exits. The C programming language is typically implemented in this way. Using the same stack for both data and procedure calls has important security implications (see below) of which a programmer must be aware in order to avoid introducing serious security bugs into a program.\nEfficient algorithms.\nSeveral algorithms use a stack (separate from the usual function call stack of most programming languages) as the principal data structure with which they organize their information. These include:\nSecurity.\nSome computing environments use stacks in ways that may make them vulnerable to security breaches and attacks. Programmers working in such environments must take special care to avoid the pitfalls of these implementations.\nFor example, some programming languages use a common stack to store both data local to a called procedure and the linking information that allows the procedure to return to its caller. This means that the program moves data into and out of the same stack that contains critical return addresses for the procedure calls. If data is moved to the wrong location on the stack, or an oversized data item is moved to a stack location that is not large enough to contain it, return information for procedure calls may be corrupted, causing the program to fail.\nMalicious parties may attempt a stack smashing attack that takes advantage of this type of implementation by providing oversized data input to a program that does not check the length of input. Such a program may copy the data in its entirety to a location on the stack, and in so doing it may change the return addresses for procedures that have called it. An attacker can experiment to find a specific type of data that can be provided to such a program such that the return address of the current procedure is reset to point to an area within the stack itself (and within the data provided by the attacker), which in turn contains instructions that carry out unauthorized operations.\nThis type of attack is a variation on the buffer overflow attack and is an extremely frequent source of security breaches in software, mainly because some of the most popular compilers use a shared stack for both data and procedure calls, and do not verify the length of data items. Frequently, programmers do not write code to verify the size of data items, either, and when an oversized or undersized data item is copied to the stack, a security breach may occur.", "categories": ["Category:Abstract data types", "Category:Articles containing German-language text", "Category:Articles with GND identifiers", "Category:Articles with example pseudocode", "Category:Articles with short description", "Category:CS1: long volume value", "Category:CS1 German-language sources (de)", "Category:CS1 location test", "Category:Commons category link is on Wikidata", "Category:Short description matches Wikidata"]}
{"docid": 1228060, "title": "Internet privacy", "text": "Internet privacy involves the right or mandate of personal privacy concerning the storing, re-purposing, provision to third parties, and displaying of information pertaining to oneself via Internet. Internet privacy is a subset of data privacy. Privacy concerns have been articulated from the beginnings of large-scale computer sharing and especially relate to mass surveillance enabled by the emergence of computer technologies.\nPrivacy can entail either personally identifiable information (PII) or non-PII information such as a site visitor's behavior on a website. PII refers to any information that can be used to identify an individual. For example, age and physical address alone could identify who an individual is without explicitly disclosing their name, as these two factors are unique enough to identify a specific person typically. Other forms of PII may soon include GPS tracking data used by apps, as the daily commute and routine information can be enough to identify an individual.\nIt has been suggested that the \"appeal of online services is to broadcast personal information on purpose.\" On the other hand, in his essay \"The Value of Privacy\", security expert Bruce Schneier says, \"Privacy protects us from abuses by those in power, even if we're doing nothing wrong at the time of surveillance.\"\nLevels of privacy.\nInternet and digital privacy are viewed differently from traditional expectations of privacy. Internet privacy is primarily concerned with protecting user information. Law Professor Jerry Kang explains that the term privacy expresses space, decision, and information. In terms of space, individuals have an expectation that their physical spaces (e.g. homes, cars) not be intruded. Information privacy is in regards to the collection of user information from a variety of sources.\nIn the United States, the 1997 Information Infrastructure Task Force (IITF) created under President Clinton defined information privacy as \"an individual's claim to control the terms under which personal information \u2014 information identifiable to the individual \u2014 is acquired, disclosed, and used.\" At the end of the 1990s, with the rise of the internet, it became clear that governments, companies, and other organizations would need to abide by new rules to protect individuals' privacy. With the rise of the internet and mobile networks internet privacy is a daily concern for users.\nPeople with only a casual concern for Internet privacy need not achieve total anonymity. Internet users may protect their privacy through controlled disclosure of personal information. The revelation of IP addresses, non-personally-identifiable profiling, and similar information might become acceptable trade-offs for the convenience that users could otherwise lose using the workarounds needed to suppress such details rigorously. On the other hand, some people desire much stronger privacy. In that case, they may try to achieve \"Internet anonymity\" to ensure privacy \u2014 use of the Internet without giving any third parties the ability to link the Internet activities to personally-identifiable information of the Internet user. In order to keep their information private, people need to be careful with what they submit to and look at online. When filling out forms and buying merchandise, information is tracked and because it was not private, some companies send Internet users spam and advertising on similar products.\nThere are also several governmental organizations that protect an individual's privacy and anonymity on the Internet, to a point. In an article presented by the FTC, in October 2011, a number of pointers were brought to attention that helps an individual internet user avoid possible identity theft and other cyber-attacks. Preventing or limiting the usage of Social Security numbers online, being wary and respectful of emails including spam messages, being mindful of personal financial details, creating and managing strong passwords, and intelligent web-browsing behaviors are recommended, among others.\nPosting things on the Internet can be harmful or expose people to malicious attacks. Some information posted on the Internet persists for decades, depending on the terms of service, and privacy policies of particular services offered online. This can include comments written on blogs, pictures, and websites, such as Facebook and Twitter. Once it is posted, anyone can potentially find it and access it. Some employers may research a potential employee by searching online for the details of their online behaviors, possibly affecting the outcome of the success of the candidate.\nRisks of Internet privacy.\nCompanies are hired to track which websites people visit and then use the information, for instance by sending advertising based on one's web browsing history. There are many ways in which people can divulge their personal information, for instance by use of \"social media\" and by sending bank and credit card information to various websites. Moreover, directly observed behavior, such as browsing logs, search queries, or contents of a Facebook profile can be automatically processed to infer potentially more intrusive details about an individual, such as sexual orientation, political and religious views, race, substance use, intelligence, and personality.\nThose concerned about Internet privacy often cite a number of \"privacy risks\" \u2014 events that can compromise privacy \u2014 which may be encountered through online activities. These range from the gathering of statistics on users to more malicious acts such as the spreading of spyware and the exploitation of various forms of bugs (software faults).\nSeveral social networking websites try to protect the personal information of their subscribers, as well as provide a warning through a privacy and terms agreement. For example, privacy settings on Facebook are available to all registered users: they can block certain individuals from seeing their profile, they can choose their \"friends\", and they can limit who has access to their pictures and videos. Privacy settings are also available on other social networking websites such as Google Plus and Twitter. The user can apply such settings when providing personal information on the Internet. The Electronic Frontier Foundation has created a set of guides so that users may more easily use these privacy settings and Zebra Crossing: an easy-to-use digital safety checklist is a volunteer-maintained online resource.\nIn late 2007, Facebook launched the Beacon program in which user rental records were released to the public for friends to see. Many people were enraged by this breach of privacy, and the \"Lane v. Facebook, Inc.\" case ensued.\nChildren and adolescents often use the Internet (including social media) in ways that risk their privacy; a cause for growing concern among parents. Young people also may not realize that all their information and browsing can and may be tracked while visiting a particular site and that it is up to them to protect their own privacy. For example, on Twitter, threats include shortened links that may lead to potentially harmful websites or content. Email threats include email scams and attachments that persuade users to install malware and disclose personal information. On Torrent sites, threats include malware hiding in video, music, and software downloads. When using a smartphone, threats include geolocation, meaning that one's phone can detect where one's location and post it online for all to see. Users can protect themselves by updating virus protection, using security settings, downloading patches, installing a firewall, screening email, shutting down spyware, controlling cookies, using encryption, fending off browser hijackers, and blocking pop-ups.\nHowever, most people have little idea how to go about doing these things. Many businesses hire professionals to take care of these issues, but most individuals can only do their best to educate themselves.\nIn 1998, the Federal Trade Commission in the US considered the lack of privacy for children on the internet and created the Children Online Privacy Protection Act (COPPA). COPPA limits the options which gather information from children and created warning labels if potential harmful information or content was presented. In 2000, the Children's Internet Protection Act (CIPA) was developed to implement Internet safety policies. Policies required taking technology protection measures that can filter or block children's Internet access to pictures that are harmful to them. Schools and libraries need to follow these requirements in order to receive discounts from E-rate program. These laws, awareness campaigns, parental and adult supervision strategies, and Internet filters can all help to make the Internet safer for children around the world.\nThe privacy concerns of Internet users pose a serious challenge (Dunkan, 1996; Till, 1997). Owing to the advancement in technology, access to the internet has become easier to use from any device at any time. However, the increase of access from multiple sources increases the number of access points for an attack. In an online survey, approximately seven out of ten individuals responded that what worries them most is their privacy over the Internet, rather than over the mail or phone. Internet privacy is slowly but surely becoming a threat, as a person's personal data may slip into the wrong hands if passed around through the Web.\nInternet protocol (IP) addresses.\nAll websites receive and many track the IP address of a visitor's computer. Companies match data over time to associate the name, address, and other information to the IP address. There is ambiguity about how private IP addresses are. The Court of Justice of the European Union has ruled they need to be treated as personally identifiable information if the website tracking them, or a third party like a service provider, knows the name or street address of the IP address holder, which would be true for static IP addresses, not for dynamic addresses.\nCalifornia regulations say IP addresses need to be treated as personal information if the business itself, not a third party, can link them to name and street address.\nAn Alberta court ruled that police can obtain the IP addresses and the names and addresses associated with them without a search warrant; the Calgary, Alberta police found IP addresses that initiated online crimes. The service provider gave police the names and addresses associated with those IP addresses.\nHTTP cookies.\nAn HTTP cookie is data stored on a user's computer that assists in automated access to websites or web features, or other state information required in complex web sites. It may also be used for user-tracking by storing special usage history data in a cookie, and such cookies \u2014 for example, those used by Google Analytics \u2014 are called \"tracking cookies\". Cookies are a common concern in the field of Internet privacy. Although website developers most commonly use cookies for legitimate technical purposes, cases of abuse occur. In 2009, two researchers noted that social networking profiles could be connected to cookies, allowing the social networking profile to be connected to browsing habits.\nIn the past, websites have not generally made the user explicitly aware of the storing of cookies, however tracking cookies and especially \"third-party tracking cookies\" are commonly used as ways to compile long-term records of individuals' browsing histories \u2014 a privacy concern that prompted European and US lawmakers to take action in 2011. Cookies can also have implications for computer forensics. In past years, most computer users were not completely aware of cookies, but users have become conscious of possible detrimental effects of Internet cookies: a recent study done has shown that 58% of users have deleted cookies from their computer at least once, and that 39% of users delete cookies from their computer every month. Since cookies are advertisers' main way of targeting potential customers, and some customers are deleting cookies, some advertisers started to use persistent Flash cookies and zombie cookies, but modern browsers and anti-malware software can now block or detect and remove such cookies.\nThe original developers of cookies intended that only the website that originally distributed cookies to users could retrieve them, therefore returning only data already possessed by the website. However, in practice programmers can circumvent this restriction. Possible consequences include:\nCookies do have benefits. One is that for websites that one frequently visits that require a password, cookies may allow a user to not have to sign in every time. A cookie can also track one's preferences to show them websites that might interest them. Cookies make more websites free to use without any type of payment. Some of these benefits are also seen as negative. For example, one of the most common ways of theft is hackers taking one's username and password that a cookie saves. While many sites are free, they sell their space to advertisers. These ads, which are personalized to one's likes, can sometimes freeze one's computer or cause annoyance. Cookies are mostly harmless except for third-party cookies. These cookies are not made by the website itself but by web banner advertising companies. These third-party cookies are dangerous because they take the same information that regular cookies do, such as browsing habits and frequently visited websites, but then they share this information with other companies.\nCookies are often associated with pop-up windows because these windows are often, but not always, tailored to a person's preferences. These windows are an irritation because the close button may be strategically hidden in an unlikely part of the screen. In the worst cases, these pop-up ads can take over the screen and while one tries to close them, they can take one to another unwanted website.\nCookies are seen so negatively because they are not understood and go unnoticed while someone is simply surfing the internet. The idea that every move one makes while on the internet is being watched, would frighten most users.\nSome users choose to disable cookies in their web browsers. Such an action can reduce some privacy risks but may severely limit or prevent the functionality of many websites. All significant web browsers have this disabling ability built-in, with no external program required. As an alternative, users may frequently delete any stored cookies. Some browsers (such as Mozilla Firefox and Opera) offer the option to clear cookies automatically whenever the user closes the browser. A third option involves allowing cookies in general but preventing their abuse. There are also a host of wrapper applications that will redirect cookies and cache data to some other location. Concerns exist that the privacy benefits of deleting cookies have been over-stated.\nThe process of \"profiling\" (also known as \"tracking\") assembles and analyzes several events, each attributable to a single originating entity, in order to gain information (especially patterns of activity) relating to the originating entity. Some organizations engage in the profiling of people's web browsing, collecting the URLs of sites visited. The resulting profiles can potentially link with information that personally identifies the individual who did the browsing.\nSome web-oriented marketing-research organizations may use this practice legitimately, for example: in order to construct profiles of \"typical internet users\". Such profiles, which describe average trends of large groups of internet users rather than of actual individuals, can then prove useful for market analysis. Although the aggregate data does not constitute a privacy violation, some people believe that the initial profiling does.\nProfiling becomes a more contentious privacy issue when data-matching associates the profile of an individual with personally-identifiable information of the individual. This is why Google, the dominant ad platform, who uses cookies to allow marketers to track people has announced plans to \"kill the cookie.\"\nGovernments and organizations may set up honeypot websites \u2013 featuring controversial topics \u2013 with the purpose of attracting and tracking unwary people. This constitutes a potential danger for individuals.\nFlash cookies.\nWhen some users choose to disable HTTP cookies to reduce privacy risks as noted, new types of cookies were invented: since cookies are advertisers' main way of targeting potential customers, and some customers were deleting cookies, some advertisers started to use persistent Flash cookies and zombie cookies. In a 2009 study, Flash cookies were found to be a popular mechanism for storing data on the top 100 most visited sites. Another 2011 study of social media found that, \"Of the top 100 web sites, 31 had at least one overlap between HTTP and Flash cookies.\" However, modern browsers and anti-malware software can now block or detect and remove such cookies.\nFlash cookies, also known as local shared objects, work the same ways as normal cookies and are used by the Adobe Flash Player to store information at the user's computer. They exhibit a similar privacy risk as normal cookies, but are not as easily blocked, meaning that the option in most browsers to not accept cookies does not affect Flash cookies. One way to view and control them is with browser extensions or add-ons.\nFlash cookies are unlike HTTP cookies in a sense that they are not transferred from the client back to the server. Web browsers read and write these cookies and can track any data by web usage.\nAlthough browsers such as Internet Explorer 8 and Firefox 3 have added a \"Privacy Browsing\" setting, they still allow Flash cookies to track the user and operate fully. However, the Flash player browser plugin can be disabled or uninstalled, and Flash cookies can be disabled on a per-site or global basis. Adobe's Flash and (PDF) Reader are not the only browser plugins whose past security defects have allowed spyware or malware to be installed: there have also been problems with Oracle's Java.\nEvercookies.\nEvercookies, created by Samy Kamkar, are JavaScript-based applications which produce cookies in a web browser that actively \"resist\" deletion by redundantly copying themselves in different forms on the user's machine (e.g., Flash Local Shared Objects, various HTML5 storage mechanisms, window.name caching, etc.), and resurrecting copies that are missing or expired. Evercookie accomplishes this by storing the cookie data in several types of storage mechanisms that are available on the local browser. It has the ability to store cookies in over ten types of storage mechanisms so that once they are on one's computer they will never be gone. Additionally, if evercookie has found the user has removed any of the types of cookies in question, it recreates them using each mechanism available. Evercookies are one type of zombie cookie. However, modern browsers and anti-malware software can now block or detect and remove such cookies.\nAnti-fraud uses.\nSome anti-fraud companies have realized the potential of evercookies to protect against and catch cyber criminals. These companies already hide small files in several places on the perpetrator's computer but hackers can usually easily get rid of these. The advantage to evercookies is that they resist deletion and can rebuild themselves.\nAdvertising uses.\nThere is controversy over where the line should be drawn on the use of this technology. Cookies store unique identifiers on a person's computer that are used to predict what one wants. Many advertisement companies want to use this technology to track what their customers are looking at online. This is known as online behavioral advertising which allows advertisers to keep track of the consumer's website visits to personalize and target advertisements. Ever-cookies enable advertisers to continue to track a customer regardless of whether their cookies are deleted or not. Some companies are already using this technology but the ethics are still being widely debated.\nCriticism.\nAnonymizer \"nevercookies\" are part of a free Firefox plugin that protects against evercookies. This plugin extends Firefox's private browsing mode so that users will be completely protected from ever-cookies. Never-cookies eliminate the entire manual deletion process while keeping the cookies users want like browsing history and saved account information.\nDevice fingerprinting.\nA \"device fingerprint\" is information collected about the software and hardware of a remote computing device for the purpose of identifying individual devices even when persistent cookies (and also zombie cookies) cannot be read or stored in the browser, the client IP address is hidden, and even if one switches to another browser on the same device.\nThis may allow a service provider to detect and prevent identity theft and credit card fraud, but also to compile long-term records of individuals' browsing histories even when they're attempting to avoid tracking, raising a major concern for internet privacy advocates.\nThird Party Requests.\nThird Party Requests are HTTP data connections from client devices to addresses in the web which are different from the website the user is currently surfing on. Many alternative tracking technologies to cookies are based on third party requests. Their importance has increased during the last years and even accelerated after Mozilla (2019), Apple (2020), and Google (2022) have announced to block third party cookies by default. Third requests may be used for embedding external content (e.g. advertisements) or for loading external resources and functions (e.g. images, icons, fonts, captchas, JQuery resources and many others). Dependent on the type of resource loaded, such requests may enable third parties to execute a device fingerprint or place any other kind of marketing tag. Irrespective of the intention, such requests do often disclose information that may be sensitive, and they can be used for tracking either directly or in combination with other personally identifiable information . Most of the requests disclose referrer details that reveal the full URL of the actually visited website. In addition to the referrer URL further information may be transmitted by the use of other request methods such as HTTP POST. Since 2018 Mozilla partially mitigates the risk of third party requests by cutting the referrer information when using the private browsing mode. However, personal information may still be revealed to the requested address in other areas of the HTTP-header.\nPhotographs on the Internet.\nToday many people have digital cameras and post their photographs online, for example street photography practitioners do so for artistic purposes and social documentary photography practitioners do so to document people in everyday life. The people depicted in these photos might not want them to appear on the Internet. Police arrest photos, considered public record in many jurisdictions, are often posted on the Internet by online mug shot publishing sites.\nSome organizations attempt to respond to this privacy-related concern. For example, the 2005 Wikimania conference required that photographers have the prior permission of the people in their pictures, albeit this made it impossible for photographers to practice candid photography and doing the same in a public place would violate the photographers' free speech rights. Some people wore a \"no photos\" tag to indicate they would prefer not to have their photo taken .\nThe \"Harvard Law Review\" published a short piece called \"In The Face of Danger: Facial Recognition and Privacy Law\", much of it explaining how \"privacy law, in its current form, is of no help to those unwillingly tagged.\" Any individual can be unwillingly tagged in a photo and displayed in a manner that might violate them personally in some way, and by the time Facebook gets to taking down the photo, many people will have already had the chance to view, share, or distribute it. Furthermore, traditional tort law does not protect people who are captured by a photograph in public because this is not counted as an invasion of privacy. The extensive Facebook privacy policy covers these concerns and much more. For example, the policy states that they reserve the right to disclose member information or share photos with companies, lawyers, courts, government entities, etc. if they feel it absolutely necessary. The policy also informs users that profile pictures are mainly to help friends connect to each other. However, these, as well as other pictures, can allow other people to invade a person's privacy by finding out information that can be used to track and locate a certain individual. In an article featured in ABC News, it was stated that two teams of scientists found out that Hollywood stars could be giving up information about their private whereabouts very easily through pictures uploaded to the internet. Moreover, it was found that pictures taken by some phones and tablets including iPhones automatically attach the latitude and longitude of the picture taken through metadata unless this function is manually disabled.\nFace recognition technology can be used to gain access to a person's private data, according to a new study. Researchers at Carnegie Mellon University combined image scanning, cloud computing and public profiles from social network sites to identify individuals in the offline world. Data captured even included a user's social security number. Experts have warned of the privacy risks faced by the increased merging of online and offline identities. The researchers have also developed an 'augmented reality' mobile app that can display personal data over a person's image captured on a smartphone screen. Since these technologies are widely available, users' future identities may become exposed to anyone with a smartphone and an internet connection. Researchers believe this could force a reconsideration of future attitudes to privacy.\nGoogle Street View.\nGoogle Street View, released in the U.S. in 2007, is currently the subject of an ongoing debate about possible infringement on individual privacy. In an article entitled \"Privacy, Reconsidered: New Representations, Data Practices, and the Geoweb\", Sarah Elwood and Agnieszka Leszczynski (2011) argue that Google Street View \"facilitate[s] identification and disclosure with more immediacy and less abstraction.\" The medium through which Street View disseminates information, the photograph, is very immediate in the sense that it can potentially provide direct information and evidence about a person's whereabouts, activities, and private property. Moreover, the technology's disclosure of information about a person is less abstract in the sense that, if photographed, a person is represented on Street View in a virtual replication of his or her own real-life appearance. In other words, the technology removes abstractions of a person's appearance or that of his or her personal belongings \u2013 there is an immediate disclosure of the person and object, as they visually exist in real life. Although Street View began to blur license plates and people's faces in 2008, the technology is faulty and does not entirely ensure against accidental disclosure of identity and private property.\nElwood and Leszczynski note that \"many of the concerns leveled at Street View stem from situations where its photograph-like images were treated as definitive evidence of an individual's involvement in particular activities.\" In one instance, Ruedi Noser, a Swiss politician, barely avoided public scandal when he was photographed in 2009 on Google Street View walking with a woman who was not his wife \u2013 the woman was actually his secretary. Similar situations occur when Street View provides high-resolution photographs \u2013 and photographs hypothetically offer compelling objective evidence. But as the case of the Swiss politician illustrates, even supposedly compelling photographic evidence is sometimes subject to gross misinterpretation. This example further suggests that Google Street View may provide opportunities for privacy infringement and harassment through public dissemination of the photographs. Google Street View does, however, blur or remove photographs of individuals and private property from image frames if the individuals request further blurring and/or removal of the images. This request can be submitted, for review, through the \"report a problem\" button that is located on the bottom left-hand side of every image window on Google Street View, however, Google has made attempts to report a problem difficult by disabling the \"Why are you reporting the street view\" icon.\nSearch engines.\nSearch engines have the ability to track a user's searches. Personal information can be revealed through searches by the user's computer, account, or IP address being linked to the search terms used. Search engines have claimed a necessity to retain such information in order to provide better services, protect against security pressure, and protect against fraud.\nA search engine takes all of its users and assigns each one a specific ID number. Those in control of the database often keep records of where on the internet each member has traveled to. AOL's system is one example. AOL has a database 21 million members deep, each with their own specific ID number. The way that AOLSearch is set up, however, allows for AOL to keep records of all the websites visited by any given member. Even though the true identity of the user is not known, a full profile of a member can be made just by using the information stored by AOLSearch. By keeping records of what people query through AOL Search, the company is able to learn a great deal about them without knowing their names.\nSearch engines also are able to retain user information, such as location and time spent using the search engine, for up to ninety days. Most search engine operators use the data to get a sense of which needs must be met in certain areas of their field. People working in the legal field are also allowed to use information collected from these search engine websites. The Google search engine is given as an example of a search engine that retains the information entered for a period of three-fourths of a year before it becomes obsolete for public usage. Yahoo! follows in the footsteps of Google in the sense that it also deletes user information after a period of ninety days. Other search engines such as Ask! search engine has promoted a tool of \"AskEraser\" which essentially takes away personal information when requested.\nSome changes made to internet search engines included that of Google's search engine. Beginning in 2009, Google began to run a new system where the Google search became personalized. The item that is searched and the results that are shown remembers previous information that pertains to the individual. Google search engine not only seeks what is searched but also strives to allow the user to feel like the search engine recognizes their interests. This is achieved by using online advertising. A system that Google uses to filter advertisements and search results that might interest the user is by having a ranking system that tests relevancy that includes observation of the behavior users exude while searching on Google. Another function of search engines is the predictability of location. Search engines are able to predict where one's location is currently by locating IP Addresses and geographical locations.\nGoogle had publicly stated on January 24, 2012, that its privacy policy will once again be altered. This new policy would change the following for its users: (1) the privacy policy would become shorter and easier to comprehend and (2) the information that users provide would be used in more ways than it is presently being used. The goal of Google is to make users\u2019 experiences better than they currently are.\nThis new privacy policy is planned to come into effect on March 1, 2012. Peter Fleischer, the Global Privacy Counselor for Google, has explained that if a person is logged into his/her Google account, and only if he/she is logged in, information will be gathered from multiple Google services in which he/she has used in order to be more accommodating. Google's new privacy policy will combine all data used on Google's search engines (i.e., YouTube and Gmail) in order to work along the lines of a person's interests. A person, in effect, will be able to find what he/she wants at a more efficient rate because all searched information during times of login will help to narrow down new search results.\nGoogle's privacy policy explains what information they collect and why they collect it, how they use the information, and how to access and update information. Google will collect information to better service its users such as their language, which ads they find useful or people that are important to them online. Google announces they will use this information to provide, maintain, protect Google and its users. The information Google uses will give users more relevant search results and advertisements. The new privacy policy explains that Google can use shared information on one service in other Google services from people who have a Google account and are logged in. Google will treat a user as a single user across all of their products. Google claims the new privacy policy will benefit its users by being simpler. Google will, for example, be able to correct the spelling of a user's friend's name in a Google search or notify a user they are late based on their calendar and current location. Even though Google is updating their privacy policy, its core privacy guidelines will not change. For example, Google does not sell personal information or share it externally.\nUsers and public officials have raised many concerns regarding Google's new privacy policy. The main concern/issue involves the sharing of data from multiple sources. Because this policy gathers all information and data searched from multiple engines when logged into Google, and uses it to help assist users, privacy becomes an important element. Public officials and Google account users are worried about online safety because of all this information being gathered from multiple sources.\nSome users do not like the overlapping privacy policy, wishing to keep the service of Google separate. The update to Google's privacy policy has alarmed both public and private sectors. The European Union has asked Google to delay the onset of the new privacy policy in order to ensure that it does not violate E.U. law. This move is in accordance with objections to decreasing online privacy raised in other foreign nations where surveillance is more heavily scrutinized. Canada and Germany have both held investigations into the legality of both Facebook, against respective privacy acts, in 2010. The new privacy policy only heightens unresolved concerns regarding user privacy.\nAn additional feature of concern to the new Google privacy policy is the nature of the policy. One must accept all features or delete existing Google accounts. The update will affect the Google+ social network, therefore making Google+\u2019s settings uncustomizable, unlike other customizable social networking sites. Customizing the privacy settings of a social network is a key tactic that many feel is necessary for social networking sites. This update in the system has some Google+ users wary of continuing service. Additionally, some fear the sharing of data amongst Google services could lead to revelations of identities. Many using pseudonyms are concerned about this possibility, and defend the role of pseudonyms in literature and history.\nSome solutions to being able to protect user privacy on the internet can include programs such as \"Rapleaf\" which is a website that has a search engine that allows users to make all of one's search information and personal information private. Other websites that also give this option to their users are Facebook and Amazon.\nPrivacy focused search engines/browsers.\nSearch engines such as Startpage.com, Disconnect.me and Scroogle (defunct since 2012) anonymize Google searches. Some of the most notable Privacy-focused search-engines are:\nPrivacy issues of social networking sites.\nThe advent of the Web 2.0 has caused social profiling and is a growing concern for internet privacy. Web 2.0 is the system that facilitates participatory information sharing and collaboration on the internet, in social networking media websites like Facebook, Instagram, Twitter, and MySpace. These social networking sites have seen a boom in their popularity starting from the late 2000s. Through these websites, many people are giving their personal information out on the internet.\nIt has been a topic of discussion of who is held accountable for the collection and distribution of personal information. Some blame social networks, because they are responsible for storing the information and data, while others blame the users who put their information on these sites. This relates to the ever-present issue of how society regards social media sites. There is a growing number of people that are discovering the risks of putting their personal information online and trusting a website to keep it private. Yet in a recent study, researchers found that young people are taking measures to keep their posted information on Facebook private to some degree. Examples of such actions include managing their privacy settings so that certain content can be visible to \"Only Friends\" and ignoring Facebook friend requests from strangers.\nIn 2013 a class action lawsuit was filed against Facebook alleging the company scanned user messages for web links, translating them to \u201clikes\u201d on the user's Facebook profile. Data lifted from the private messages was then used for targeted advertising, the plaintiffs claimed. \"Facebook's practice of scanning the content of these messages violates the federal Electronic Communications Privacy Act (ECPA also referred to as the Wiretap Act), as well as California's Invasion of Privacy Act (CIPA), and section 17200 of California's Business and Professions Code,\" the plaintiffs said. This shows that once information is online it is no longer completely private. It is an increasing risk because younger people are having easier internet access than ever before, therefore they put themselves in a position where it is all too easy for them to upload information, but they may not have the caution to consider how difficult it can be to take that information down once it has been out in the open. This is becoming a bigger issue now that so much of society interacts online which was not the case fifteen years ago. In addition, because of the quickly evolving digital media arena, people's interpretation of privacy is evolving as well, and it is important to consider that when interacting online. New forms of social networking and digital media such as Instagram and Snapchat may call for new guidelines regarding privacy. What makes this difficult is the wide range of opinions surrounding the topic, so it is left mainly up to individual judgement to respect other people's online privacy in some circumstances.\nPrivacy issues of medical applications.\nWith the rise of technology focused applications, there has been a rise of medical apps available to users on smart devices. In a survey of 29 migraine management specific applications, researcher Mia T. Minen (et al.) discovered 76% had clear privacy policies, with 55% of the apps stated using the user data from these giving data to third parties for the use of advertising. The concerns raised discusses the applications without accessible privacy policies, and even more so - applications that are not properly adhering to the Health Insurance Portability and Accountability Act (HIPAA) are in need of proper regulation, as these apps store medical data with identifiable information on a user.\nInternet service providers.\nInternet users obtain internet access through an internet service provider (ISP). All data transmitted to and from users must pass through the ISP. Thus, an ISP has the potential to observe users' activities on the internet. ISPs can breach personal information such as transaction history, search history, and social media profiles of users. Hackers could use this opportunity to hack ISP and obtain sensitive information of victims.\nHowever, ISPs are usually prohibited from participating in such activities due to legal, ethical, business, or technical reasons.\nNormally ISPs do collect at least \"some\" information about the consumers using their services. From a privacy standpoint, ISPs would ideally collect only as much information as they require in order to provide internet connectivity (IP address, billing information if applicable, etc.).\nWhich information an ISP collects, what it does with that information, and whether it informs its consumers, pose significant privacy issues. Beyond the usage of collected information typical of third parties, ISPs sometimes state that they will make their information available to government authorities upon request. In the US and other countries, such a request does not necessarily require a warrant.\nAn ISP cannot know the contents of properly encrypted data passing between its consumers and the internet. For encrypting web traffic, https has become the most popular and best-supported standard. Even if users encrypt the data, the ISP still knows the IP addresses of the sender and of the recipient. (However, see the IP addresses section for workarounds.)\nAn Anonymizer such as I2P \u2013 The Anonymous Network or Tor can be used for accessing web services without them knowing one's IP address and without one's ISP knowing what the services are that one accesses. Additional software has been developed that may provide more secure and anonymous alternatives to other applications. For example, Bitmessage can be used as an alternative for email and Cryptocat as an alternative for online chat. On the other hand, in addition to End-to-End encryption software, there are web services such as Qlink which provide privacy through a novel security protocol which does not require installing any software.\nWhile signing up for internet services, each computer contains a unique IP, Internet Protocol address. This particular address will not give away private or personal information, however, a weak link could potentially reveal information from one's ISP.\nGeneral concerns regarding internet user privacy have become enough of a concern for a UN agency to issue a report on the dangers of identity fraud. In 2007, the Council of Europe held its first annual Data Protection Day on January 28, which has since evolved into the annual Data Privacy Day.\nT-Mobile USA does not store any information on web browsing. Verizon Wireless keeps a record of the websites a subscriber visits for up to a year. Virgin Mobile keeps text messages for three months. Verizon keeps text messages for three to five days. None of the other carriers keep texts of messages at all, but they keep a record of who texted who for over a year. AT&T Mobility keeps for five to seven years a record of who text messages who and the date and time, but not the content of the messages. Virgin Mobile keeps that data for two to three months.\nHTML5.\nHTML5 is the latest version of Hypertext Markup Language specification. HTML defines how user agents, such as web browsers, are to present websites based upon their underlying code. This new web standard changes the way that users are affected by the internet and their privacy on the internet. HTML5 expands the number of methods given to a website to store information locally on a client as well as the amount of data that can be stored. As such, privacy risks are increased. For instance, merely erasing cookies may not be enough to remove potential tracking methods since data could be mirrored in web storage, another means of keeping information in a user's web browser. There are so many sources of data storage that it is challenging for web browsers to present sensible privacy settings. As the power of web standards increases, so do potential misuses.\nHTML5 also expands access to user media, potentially granting access to a computer's microphone or webcam, a capability previously only possible through the use of plug-ins like Flash. It is also possible to find a user's geographical location using the geolocation API. With this expanded access comes increased potential for abuse as well as more vectors for attackers. If a malicious site was able to gain access to a user's media, it could potentially use recordings to uncover sensitive information thought to be unexposed. However, the World Wide Web Consortium, responsible for many web standards, feels that the increased capabilities of the web platform outweigh potential privacy concerns. They state that by documenting new capabilities in an open standardization process, rather than through closed source plug-ins made by companies, it is easier to spot flaws in specifications and cultivate expert advice.\nBesides elevating privacy concerns, HTML5 also adds a few tools to enhance user privacy. A mechanism is defined whereby user agents can share blacklists of domains that should not be allowed to access web storage. Content Security Policy is a proposed standard whereby sites may assign privileges to different domains, enforcing harsh limitations on JavaScript use to mitigate cross-site scripting attacks. HTML5 also adds HTML templating and a standard HTML parser which replaces the various parsers of web browser vendors. These new features formalize previously inconsistent implementations, reducing the number of vulnerabilities though not eliminating them entirely.\nBig data.\nBig data is generally defined as the rapid accumulation and compiling of massive amounts of information that is being exchanged over digital communication systems. The volume of data is large (often exceeding exabytes), cannot be handled by conventional computer processors, and is instead stored on large server-system databases. This information is assessed by analytic scientists using software programs, which paraphrase this information into multi-layered user trends and demographics. This information is collected from all around the internet, such as by popular services like Facebook, Google, Apple, Spotify or GPS systems.\nBig data provides companies with the ability to:\nReduction of risks to Internet privacy.\n\"Inc.\" magazine reports that the Internet's biggest corporations have hoarded Internet users' personal data and sold it for large financial profits.\nPrivate mobile messaging.\nThe magazine reports on a band of startup companies that are demanding privacy and aiming to overhaul the social-media business. Popular privacy-focused mobile messaging apps include Wickr, Wire, and Signal, which provide peer-to-peer encryption and give the user the capacity to control what message information is retained on the other end.\nProtection through information overflow.\nAccording to Nicklas Lundblad, another perspective on privacy protection is the assumption that the quickly growing amount of information produced will be beneficial. The reasons for this are that the costs for the surveillance will raise and that there is more noise, noise being understood as anything that interferes the process of a receiver trying to extract private data from a sender.\nIn this noise society, the collective expectation of privacy will increase, but the individual expectation of privacy will decrease. In other words, not everyone can be analyzed in detail, but one individual can be. Also, in order to stay unobserved, it can hence be better to blend in with the others than trying to use for example encryption technologies and similar methods. Technologies for this can be called Jante-technologies after the Law of Jante, which states that you are nobody special. This view offers new challenges and perspectives for the privacy discussion.\nPublic views.\nWhile internet privacy is widely acknowledged as the top consideration in any online interaction, as evinced by the public outcry over SOPA/CISPA, public understanding of online privacy policies is actually being negatively affected by the current trends regarding online privacy statements. Users have a tendency to skim internet privacy policies for information regarding the distribution of personal information only, and the more legalistic the policies appear, the less likely users are to even read the information. Coupling this with the increasingly exhaustive license agreements companies require consumers to agree to before using their product, consumers are reading less about their rights.\nFurthermore, if the user has already done business with a company, or is previously familiar with a product, they have a tendency to not read the privacy policies that the company has posted. As internet companies become more established, their policies may change, but their clients will be less likely to inform themselves of the change. This tendency is interesting because as consumers become more acquainted with the internet they are also more likely to be interested in online privacy. Finally, consumers have been found to avoid reading the privacy policies if the policies are not in a simple format, and even perceive these policies to be irrelevant. The less readily available terms and conditions are, the less likely the public is to inform themselves of their rights regarding the service they are using.\nConcerns of internet privacy and real-life implications.\nWhile dealing with the issue of internet privacy, one must first be concerned with not only the technological implications such as damaged property, corrupted files, and the like, but also with the potential for implications on their real lives. One such implication, which is rather commonly viewed as being one of the most daunting fears risks of the internet, is the potential for identity theft. Although it is a typical belief that larger companies and enterprises are the usual focus of identity thefts, rather than individuals, recent reports seem to show a trend opposing this belief. Specifically, it was found in a 2007 \"Internet Security Threat Report\" that roughly ninety-three percent of \"gateway\" attacks were targeted at unprepared home users. The term \"gateway attack\" was used to refer to an attack which aimed not at stealing data immediately, but rather at gaining access for future attacks.\nAccording to Symantec's \"Internet Security Threat Report\", this continues despite the increasing emphasis on internet security due to the expanding \"underground economy\". With more than fifty percent of the supporting servers located in the United States, this underground economy has become a haven for internet thieves, who use the system in order to sell stolen information. These pieces of information can range from generic things such as a user account or email to something as personal as a bank account number and PIN.\nWhile the processes these internet thieves use are abundant and unique, one popular trap people fall into is that of online purchasing. In a 2001 article titled \"Consumer Watch\", the popular online site PC World went called secure e-shopping a myth. Though unlike the gateway attacks mentioned above, these incidents of information being stolen through online purchases generally are more prevalent in medium to large e-commerce sites, rather than smaller individualized sites. This is assumed to be a result of the larger consumer population and purchases, which allow for more potential leeway with information.\nUltimately, however, the potential for a violation of one's privacy is typically out of their hands after purchasing from an online \"e-tailer\" or store. One of the most common forms in which hackers receive private information from online e-tailers actually comes from an attack placed upon the site's servers responsible for maintaining information about previous transactions. For as experts explain, these e-tailers are not doing nearly enough to maintain or improve their security measures. Even those sites that clearly present a privacy or security policy can be subject to hackers\u2019 havoc as most policies only rely upon encryption technology which only applies to the actual transfer of a customer's data. However, with this being said, most e-tailers have been making improvements, going as far as covering some of the credit card fees if the information's abuse can be traced back to the site's servers.\nAs one of the largest growing concerns American adults have of current internet privacy policies, identity and credit theft remain a constant figure in the debate surrounding privacy online. A 1997 study by the Boston Consulting Group showed that participants of the study were most concerned about their privacy on the internet compared to any other media. However, it is important to recall that these issues are not the only prevalent concerns society has. Another prevalent issue remains members of society sending disconcerting emails to one another. It is for this reason in 2001 that for one of the first times the public expressed approval of government intervention in their private lives.\nWith the overall public anxiety regarding the constantly expanding trend of online crimes, in 2001 roughly fifty-four percent of Americans polled showed a general approval for the FBI monitoring those emails deemed suspicious. Thus, it was born the idea for the FBI program: \"Carnivore\", which was going to be used as a searching method, allowing the FBI to hopefully home in on potential criminals. Unlike the overall approval of the FBI's intervention, Carnivore was not met with as much of a majority's approval. Rather, the public seemed to be divided with forty-five percent siding in its favor, forty-five percent opposed to the idea for its ability to potentially interfere with ordinary citizen's messages, and ten percent claiming indifference. While this may seem slightly tangent to the topic of internet privacy, it is important to consider that at the time of this poll, the general population's approval on government actions was declining, reaching thirty-one percent versus the forty-one percent it held a decade prior. This figure in collaboration with the majority's approval of FBI intervention demonstrates an emerging emphasis on the issue of internet privacy in society and more importantly, the potential implications it may hold on citizens\u2019 lives.\nOnline users must seek to protect the information they share with online websites, specifically social media. In today's Web 2.0 individuals have become the public producers of personal information. Users create their own digital trails that hackers and companies alike capture and utilize for a variety of marketing and advertisement targeting. A recent paper from the Rand Corporation claims \"privacy is not the opposite of sharing \u2013 rather, it is control over sharing.\" Internet privacy concerns arise from the surrender of personal information to engage in a variety of acts, from transactions to commenting in online forums. Protection against invasions of online privacy will require individuals to make an effort informing and protecting themselves via existing software solutions, to pay premiums for such protections or require individuals to place greater pressure on governing institutions to enforce privacy laws and regulations regarding consumer and personal information.\nImpact of internet surveillance tools on marginalized communities.\nInternet privacy issues also affect existing class distinctions in the United States, often disproportionately impacting historically marginalized groups typically classified by race and class. Individuals with access to private digital connections that have protective services are able to more easily prevent data privacy risks of personal information and surveillance issues. Members of historically marginalized communities face greater risks of surveillance through the process of data profiling, which increases the likelihood of being stereotyped, targeted, and exploited, thus exacerbating pre-existing inequities that foster uneven playing fields. There are severe, and often unintentional, implications for big data which results in data profiling. For example, automated systems of employment verification run by the federal government such as E-verify tend to misidentify people with names that do not adhere to standardized Caucasian-sounding names as ineligible to work in the United States, thus widening unemployment gaps and preventing social mobility. This case exemplifies how some programs have bias embedded within their codes.\nTools using algorithms and artificial intelligence have also been used to target marginalized communities with policing measures, such as using facial recognition softwares and predictive policing technologies that use data to predict where a crime will most likely occur, and who will engage in the criminal activity. Studies have shown that these tools exacerbate the existing issue of over-policing in areas that are predominantly home to marginalized groups. These tools and other means of data collection can also prohibit historically marginalized and low-income groups from financial services regulated by the state, such as securing loans for house mortgages. Black applicants are rejected by mortgage and mortgage refinancing services at a much higher rate than white people, exacerbating existing racial divisions. Members of minority groups have lower incomes and lower credit scores than white people, and often live in areas with lower home values. Another example of technologies being used for surveilling practices is seen in immigration. Border control systems often use artificial intelligence in facial recognition systems, fingerprint scans, ground sensors, aerial video surveillance machines, and decision-making in asylum determination processes. This has led to large-scale data storage and physical tracking of refugees and migrants.\nWhile broadband was implemented as a means to transform the relationship between historically marginalized communities and technology to ultimately narrow the digital inequalities, inadequate privacy protections compromise user rights, profile users, and spur skepticism towards technology among users. Some automated systems, like the United Kingdom government\u2019s Universal Credit system in 2013, have failed to take into account that people, often minorities, may already lack internet access or digital literacy skills and therefore be deemed ineligible for online identity verification requirements, such as forms for job applications or to receive social security benefits, for example. Marginalized communities using broadband services may also not be aware of how digital information flows and is shared with powerful media conglomerates, reflecting a broader sense of distrust and fear these communities have with the state. Marginalized communities may therefore end up feeling dissatisfied or targeted by broadband services, whether from nonprofit community service providers or state providers.\nLaws and regulations.\nGlobal privacy policies.\nThe General Data Protection Regulation (GDPR) is the toughest privacy and security law in the world. Though it was drafted and passed by the European Union (EU), it imposes obligations onto organizations anywhere, so long as they target or collect data related to people in the EU. There are no globally unified laws and regulations.\nEuropean General Data protection regulation.\nIn 2009 the European Union has for the first time created awareness on tracking practices when the ePrivacy-Directive (2009/136/EC) was put in force. In order to comply with this directive, websites had to actively inform the visitor about the use of cookies. This disclosure has been typically implemented by showing small information banners. 9 years later, by 25 May 2018 the European General Data Protection Regulation (GDPR) came in force, which targets to regulate and restrict the usage of personal data in general, irrespective of how the information is being processed. The regulation primarily applies to so-called \u201ccontrollers\u201d, which are (a) all organizations that process personal information within the European Union, and (b) all organizations which process personal information of EU-based persons outside the European Union. Article 4 (1) defines personal information as anything that may be used for identifying a \u201cdata subject\u201d (e.g. natural person) either directly or in combination with other personal information. In theory this even takes common internet identifiers such as cookies or IP-Addresses in scope of this regulation. Processing such personal information is restricted unless a \"lawful reason\" according to Article 6 (1) applies. The most important lawful reason for data processing on the internet is the explicit content given by the data subject. More strict requirements apply for sensitive personal information (Art 9), which may be used for revealing information about ethnic origin, political opinion, religion, trade union membership, biometrics, health or sexual orientation. However, explicit user content still is sufficient to process such sensitive personal information (Art 9 (2) lit a). \u201cExplicit consent\u201d requires an affirmative act (Art 4 (11)), which is given if the individual person is able to freely choose and does consequently actively opt in.\nAs per June 2020, typical cookie implementations are not compliant to this regulation, and other practices such as device fingerprinting, cross-website-logins or 3rd party-requests are typically not disclosed, even though many opinions consider such methods in scope of the GDPR. The reason for this controversy is the ePrivacy-Directive 2009/136/EC which is still unchanged in force. An updated version of this directive, formulated as ePrivacy Regulation, shall enlarge the scope from cookies only to any type of tracking method. It shall furthermore cover any kind of electronic communication channels such as Skype or WhatsApp. The new ePrivacy-Regulation was planned to come in force together with the GDPR, but as per July 2020 it was still under review. Some people assume that lobbying is the reason for this massive delay.\nIrrespective of the pending ePrivacy-Regulation, the European High Court has decided in October 2019 (case C-673/17) that the current law is not fulfilled if the disclosed information in the cookie disclaimer is imprecise, or if the consent checkbox is pre-checked. Consequently, many cookie disclaimers that were in use at that time were confirmed to be incompliant to the current data protection laws. However, even this high court judgement only refers to cookies and not to other tracking methods.\nInternet privacy in China.\nOne of the most popular topics of discussion in regards to internet privacy is China. Although China is known for its remarkable reputation on maintaining internet privacy among many online users, it could potentially be a major jeopardy to the lives of many online users who have their information exchanged on the web on a regular basis. For instance, in China, there is a new software that will enable the concept of surveillance among the majority of online users and present a risk to their privacy. The main concern with privacy of internet users in China is the lack thereof. China has a well known policy of censorship when it comes to the spread of information through public media channels. Censorship has been prominent in Mainland China since the communist party gained power in China over 60 years ago. With the development of the internet, however, privacy became more of a problem for the government. The Chinese Government has been accused of actively limiting and editing the information that flows into the country via various media. The internet poses a particular set of issues for this type of censorship, especially when search engines are involved. Yahoo! for example, encountered a problem after entering China in the mid-2000s. A Chinese journalist, who was also a Yahoo! user, sent private emails using the Yahoo! server regarding the Chinese government. Yahoo! provided information to the Chinese government officials track down journalist, Shi Tao. Shi Tao allegedly posted state secrets to a New York-based website. Yahoo provided incriminating records of the journalist's account logins to the Chinese government and thus, Shi Tao was sentenced to ten years in prison. These types of occurrences have been reported numerous times and have been criticized by foreign entities such as the creators of the Tor network, which was designed to circumvent network surveillance in multiple countries.\nUser privacy in China is not as cut-and-dry as it is in other parts of the world. China, reportedly, has a much more invasive policy when internet activity involves the Chinese government. For this reason, search engines are under constant pressure to conform to Chinese rules and regulations on censorship while still attempting to keep their integrity. Therefore, most search engines operate differently in China than in other countries, such as the US or Britain, if they operate in China at all. There are two types of intrusions that occur in China regarding the internet: the alleged intrusion of the company providing users with internet service, and the alleged intrusion of the Chinese government. The intrusion allegations made against companies providing users with internet service are based upon reports that companies, such as Yahoo! in the previous example, are using their access to the internet users' private information to track and monitor users' internet activity. Additionally, there have been reports that personal information has been sold. For example, students preparing for exams would receive calls from unknown numbers selling school supplies. The claims made against the Chinese government lie in the fact that the government is forcing internet-based companies to track users private online data without the user knowing that they are being monitored. Both alleged intrusions are relatively harsh and possibly force foreign internet service providers to decide if they value the Chinese market over internet privacy. Also, many websites are blocked in China such as Facebook and Twitter. However many Chinese internet users use special methods like a VPN to unblock websites that are blocked.\nInternet privacy in Sweden.\nSweden is considered to be at the forefront of internet use and regulations. On 11 May 1973 Sweden enacted the Data Act \u2212 the world's first national data protection law. They are constantly innovating the way that the internet is used and how it impacts their people. In 2012, Sweden received a Web Index Score of 100, a score that measures how the internet significantly influences political, social, and economic impact, placing them first among 61 other nations. Sweden received this score while in the process of exceeding new mandatory implementations from the European Union. Sweden placed more restrictive guidelines on the directive on intellectual property rights enforcement (IPRED) and passed the FRA-law in 2009 that allowed for the legal sanctioning of surveillance of internet traffic by state authorities. The FRA has a history of intercepting radio signals and has stood as the main intelligence agency in Sweden since 1942. Sweden has a mixture of government's strong push towards implementing policy and citizens' continued perception of a free and neutral internet. Both of the previously mentioned additions created controversy by critics but they did not change the public perception even though the new FRA-law was brought in front of the European Court of Human Rights for human rights violations. The law was established by the National Defense Radio Establishment (Forsvarets Radio Anstalt - FRA) to eliminate outside threats. However, the law also allowed for authorities to monitor all cross-border communication without a warrant. Sweden's recent emergence into internet dominance may be explained by their recent climb in users. Only 2% of all Swedes were connected to the internet in 1995 but at last count in 2012, 89% had broadband access. This was due in large part once again to the active Swedish government introducing regulatory provisions to promote competition among internet service providers. These regulations helped grow web infrastructure and forced prices below the European average.\nFor copyright laws, Sweden was the birthplace of the Pirate Bay, an infamous file-sharing website. File sharing has been illegal in Sweden since it was developed, however, there was never any real fear of being persecuted for the crime until 2009 when the Swedish Parliament was the first in the European Union to pass the intellectual property rights directive. This directive persuaded internet service providers to announce the identity of suspected violators.\nSweden also has its infamous centralized block list. The list is generated by authorities and was originally crafted to eliminate sites hosting child pornography. However, there is no legal way to appeal a site that ends up on the list and as a result, many non-child pornography sites have been blacklisted. Sweden's government enjoys a high level of trust from their citizens. Without this trust, many of these regulations would not be possible and thus many of these regulations may only be feasible in the Swedish context.\nInternet privacy in the United States.\nAndrew Grove, co-founder and former CEO of Intel Corporation, offered his thoughts on internet privacy in an interview published in May 2000:\nMore than two decades later, Susan Ariel Aaronson, director of the Digital Trade and Data Governance Hub at George Washington University observed, in 2022, that:\nOverview.\nWith the Republicans in control of all three branches of the U.S. government, lobbyists for internet service providers (ISPs) and tech firms persuaded lawmakers to dismantle regulations to protect privacy which had been made during the Obama administration. These FCC rules had required ISPs to get \"explicit consent\" before gathering and selling their private internet information, such as the consumers' browsing histories, locations of businesses visited and applications used. Trade groups wanted to be able to sell this information for profit. Lobbyists persuaded Republican senator Jeff Flake and Republican representative Marsha Blackburn to sponsor legislation to dismantle internet privacy rules; Flake received $22,700 in donations and Blackburn received $20,500 in donations from these trade groups. On March 23, 2017, abolition of these privacy protections passed on a narrow party-line vote. In June 2018, California passed the law restricting companies from sharing user data without permission. Also, users would be informed to whom the data is being sold and why. On refusal to sell the data, companies are allowed to charge a little higher to these consumers. Mitt Romney, despite approving a Twitter comment of Mark Cuban during a conversation with Glenn Greenwald about anonymity in January 2018, was revealed as the owner of the Pierre Delecto lurker account in October 2019.\nLegal threats.\nUsed by government agencies are array of technologies designed to track and gather internet users' information are the topic of much debate between privacy advocates, civil liberties advocates and those who believe such measures are necessary for law enforcement to keep pace with rapidly changing communications technology.\nSpecific examples:\nChildren and internet privacy.\nInternet privacy is a growing concern with children and the content they are able to view. Aside from that, many concerns for the privacy of email, the vulnerability of internet users to have their internet usage tracked, and the collection of personal information also exist. These concerns have begun to bring the issues of internet privacy before the courts and judges.", "categories": ["Category:All Wikipedia articles in need of updating", "Category:All Wikipedia neutral point of view disputes", "Category:All articles needing additional references", "Category:All articles that may contain original research", "Category:All articles with specifically marked weasel-worded phrases", "Category:All articles with style issues", "Category:All articles with unsourced statements", "Category:Articles needing additional references from June 2014", "Category:Articles that may contain original research from November 2022", "Category:Articles with excerpts"]}
{"docid": 2356314, "title": "Open-source software development", "text": "Open-source software development (OSSD) is the process by which open-source software, or similar software whose source code is publicly available, is developed by an open-source software project. These are software products available with its source code under an open-source license to study, change, and improve its design. Examples of some popular open-source software products are Mozilla Firefox, Google Chromium, Android, LibreOffice and the VLC media player.\nHistory.\nIn 1997, Eric S. Raymond wrote \"The Cathedral and the Bazaar\". In this book, Raymond makes the distinction between two kinds of software development. The first is the conventional closed-source development. This kind of development method is, according to Raymond, like the building of a cathedral; central planning, tight organization and one process from start to finish. The second is the progressive open-source development, which is more like \"a great babbling bazaar of differing agendas and approaches out of which a coherent and stable system could seemingly emerge only by a succession of miracles.\" The latter analogy points to the discussion involved in an open-source development process.\nDifferences between the two styles of development, according to Bar and Fogel, are in general the handling (and creation) of bug reports and feature requests, and the constraints under which the programmers are working. In closed-source software development, the programmers are often spending a lot of time dealing with and creating bug reports, as well as handling feature requests. This time is spent on creating and prioritizing further development plans. This leads to part of the development team spending a lot of time on these issues, and not on the actual development. Also, in closed-source projects, the development teams must often work under management-related constraints (such as deadlines, budgets, etc.) that interfere with technical issues of the software. In open-source software development, these issues are solved by integrating the users of the software in the development process, or even letting these users build the system themselves.\nModel.\nOpen-source software development can be divided into several phases. The phases specified here are derived from \"Sharma et al\". A diagram displaying the process-data structure of open-source software development is shown on the right. In this picture, the phases of open-source software development are displayed, along with the corresponding data elements. This diagram is made using the meta-modeling and meta-process modeling techniques.\nStarting an open-source project.\nThere are several ways in which work on an open-source project can start:\nEric Raymond observed in his essay \"The Cathedral and the Bazaar\" that announcing the intent for a project is usually inferior to releasing a working project to the public.\nIt's a common mistake to start a project when contributing to an existing similar project would be more effective (NIH syndrome). To start a successful project it is very important to investigate what's already there. The process starts with a choice between the adopting of an existing project, or the starting of a new project. If a new project is started, the process goes to the Initiation phase. If an existing project is adopted, the process goes directly to the Execution phase.\nTypes of open-source projects.\nSeveral types of open-source projects exist. First, there is the garden variety of software programs and libraries, which consist of standalone pieces of code. Some might even be dependent on other open-source projects. These projects serve a specified purpose and fill a definite need. Examples of this type of project include the Linux kernel, the Firefox web browser and the LibreOffice office suite of tools.\nDistributions are another type of open-source project. Distributions are collections of software that are published from the same source with a common purpose. The most prominent example of a \"distribution\" is an operating system. There are many Linux distributions (such as Debian, Fedora Core, Mandriva, Slackware, Ubuntu etc.) which ship the Linux kernel along with many user-land components. There are other distributions, like ActivePerl, the Perl programming language for various operating systems, and Cygwin distributions of open-source programs for Microsoft Windows.\nOther open-source projects, like the BSD derivatives, maintain the source code of an entire operating system, the kernel and all of its core components, in one revision control system; developing the entire system together as a single team. These operating system development projects closely integrate their tools, more so than in the other distribution-based systems.\nFinally, there is the book or standalone document project. These items usually do not ship as part of an open-source software package. The Linux Documentation Project hosts many such projects that document various aspects of the Linux operating system. There are many other examples of this type of open-source project.\nMethods.\nIt is hard to run an open-source project following a more traditional software development method like the waterfall model, because in these traditional methods it is not allowed to go back to a previous phase. In open-source software development, requirements are rarely gathered before the start of the project; instead they are based on early releases of the software product, as Robbins describes. Besides requirements, often volunteer staff is attracted to help develop the software product based on the early releases of the software. This networking effect is essential according to Abrahamsson et al.: \u201cif the introduced prototype gathers enough attention, it will gradually start to attract more and more developers\u201d. However, Abrahamsson et al. also point out that the community is very harsh, much like the business world of closed-source software: \u201cif you find the customers you survive, but without customers you die\u201d.\nFuggetta argues that \u201crapid prototyping, incremental and evolutionary development, spiral lifecycle, rapid application development, and, recently, extreme programming and the agile software process can be equally applied to proprietary and open source software\u201d. He also pinpoints Extreme Programming as an extremely useful method for open source software development. More generally, all Agile programming methods are applicable to open-source software development, because of their iterative and incremental character. Other Agile method are equally useful for both open and closed source software development:Internet-Speed Development, for example is suitable for open-source software development because of the distributed development principle it adopts. Internet-Speed Development uses geographically distributed teams to \u2018work around the clock\u2019. This method, mostly adopted by large closed-source firms, (because they're the only ones which afford development centers in different time zones), works equally well in open source projects because a software developed by a large group of volunteers shall naturally tend to have developers spread across all time zones.\nTools.\nCommunication channels.\nDevelopers and users of an open-source project are not all necessarily working on the project in proximity. They require some electronic means of communications. E-mail is one of the most common forms of communication among open-source developers and users. Often, electronic mailing lists are used to make sure e-mail messages are delivered to all interested parties at once. This ensures that at least one of the members can reply to it. In order to communicate in real time, many projects use an instant messaging method such as IRC. Web forums have recently become a common way for users to get help with problems they encounter when using an open-source product. Wikis have become common as a communication medium for developers and users.\nVersion control systems.\nIn OSS development the participants, who are mostly volunteers, are distributed amongst different geographic regions so there is need for tools to aid participants to collaborate in the development of source code.\nDuring early 2000s, Concurrent Versions System (CVS) was a prominent example of a source code collaboration tool being used in OSS projects. CVS helps manage the files and codes of a project when several people are working on the project at the same time. CVS allows several people to work on the same file at the same time. This is done by moving the file into the users\u2019 directories and then merging the files when the users are done. CVS also enables one to easily retrieve a previous version of a file. During mid 2000s, The Subversion revision control system (SVN) was created to replace CVS. It is quickly gaining ground as an OSS project version control system.\nMany open-source projects are now using distributed revision control systems, which scale better than centralized repositories such as SVN and CVS. Popular examples are git, used by the Linux kernel, and Mercurial, used by the Python programming language.\nBug trackers and task lists.\nMost large-scale projects require a bug tracking system to keep track of the status of various issues in the development of the project.\nTesting and debugging tools.\nSince OSS projects undergo frequent integration, tools that help automate testing during system integration are used. An example of such tool is Tinderbox. Tinderbox enables participants in an OSS project to detect errors during system integration. Tinderbox runs a continuous build process and informs users about the parts of source code that have issues and on which platform(s) these issues arise.\nA debugger is a computer program that is used to debug (and sometimes test or optimize) other programs. GNU Debugger (GDB) is an example of a debugger used in open-source software development. This debugger offers remote debugging, what makes it especially applicable to open-source software development.\nA memory leak tool or memory debugger is a programming tool for finding memory leaks and buffer overflows. A memory leak is a particular kind of unnecessary memory consumption by a computer program, where the program fails to release memory that is no longer needed. Examples of memory leak detection tools used by Mozilla are the XPCOM Memory Leak tools.\nValidation tools are used to check if pieces of code conform to the specified syntax. An example of a validation tool is Splint.\nPackage management.\nA package management system is a collection of tools to automate the process of installing, upgrading, configuring, and removing software packages from a computer. The Red Hat Package Manager (RPM) for .rpm and Advanced Packaging Tool (APT) for .deb file format, are package management systems used by a number of Linux distributions.\nPublicizing a project.\nSoftware directories and release logs:\nArticles:", "categories": ["Category:All articles needing additional references", "Category:All articles that may contain original research", "Category:All articles with unsourced statements", "Category:Articles needing additional references from May 2013", "Category:Articles that may contain original research from October 2019", "Category:Articles with short description", "Category:Articles with unsourced statements from November 2014", "Category:Articles with unsourced statements from October 2019", "Category:Free software", "Category:Short description matches Wikidata"]}
{"docid": 7933386, "title": "CUDA", "text": "CUDA (or Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) that allows software to use certain types of graphics processing units (GPUs) for general purpose processing, an approach called general-purpose computing on GPUs (GPGPU). CUDA is a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements, for the execution of compute kernels.\nCUDA is designed to work with programming languages such as C, C++, and Fortran. This accessibility makes it easier for specialists in parallel programming to use GPU resources, in contrast to prior APIs like Direct3D and OpenGL, which required advanced skills in graphics programming. CUDA-powered GPUs also support programming frameworks such as OpenMP, OpenACC and OpenCL; and HIP by compiling such code to CUDA.\nCUDA was created by Nvidia. When it was first introduced, the name was an acronym for Compute Unified Device Architecture, but Nvidia later dropped the common use of the acronym.\nBackground.\nThe graphics processing unit (GPU), as a specialized computer processor, addresses the demands of real-time high-resolution 3D graphics compute-intensive tasks. By 2012, GPUs had evolved into highly parallel multi-core systems allowing efficient manipulation of large blocks of data. This design is more effective than general-purpose central processing unit (CPUs) for algorithms in situations where processing large blocks of data is done in parallel, such as:\nOntology.\nThe following table offers a non-exact description for the ontology of CUDA framework.\nProgramming abilities.\nThe CUDA platform is accessible to software developers through CUDA-accelerated libraries, compiler directives such as OpenACC, and extensions to industry-standard programming languages including C, C++ and Fortran. C/C++ programmers can use 'CUDA C/C++', compiled to PTX with nvcc, Nvidia's LLVM-based C/C++ compiler, or by clang itself. Fortran programmers can use 'CUDA Fortran', compiled with the PGI CUDA Fortran compiler from The Portland Group.\nIn addition to libraries, compiler directives, CUDA C/C++ and CUDA Fortran, the CUDA platform supports other computational interfaces, including the Khronos Group's OpenCL, Microsoft's DirectCompute, OpenGL Compute Shader and C++ AMP. Third party wrappers are also available for Python, Perl, Fortran, Java, Ruby, Lua, Common Lisp, Haskell, R, MATLAB, IDL, Julia, and native support in Mathematica.\nIn the computer game industry, GPUs are used for graphics rendering, and for game physics calculations (physical effects such as debris, smoke, fire, fluids); examples include PhysX and Bullet. CUDA has also been used to accelerate non-graphical applications in computational biology, cryptography and other fields by an order of magnitude or more.\nCUDA provides both a low level API (CUDA Driver API, non single-source) and a higher level API (CUDA Runtime API, single-source). The initial CUDA SDK was made public on 15 February 2007, for Microsoft Windows and Linux. Mac OS X support was later added in version 2.0, which supersedes the beta released February 14, 2008. CUDA works with all Nvidia GPUs from the G8x series onwards, including GeForce, Quadro and the Tesla line. CUDA is compatible with most standard operating systems.\nCUDA 8.0 comes with the following libraries (for compilation & runtime, in alphabetical order):\nCUDA 8.0 comes with these other software components:\nCUDA 9.0\u20139.2 comes with these other components:\nCUDA 10 comes with these other components:\nCUDA 11.0-11.8 comes with these other components:\nAdvantages.\nCUDA has several advantages over traditional general-purpose computation on GPUs (GPGPU) using graphics APIs:\nExample.\nThis example code in C++ loads a texture from an image into an array on the GPU:\ntexture tex;\nvoid foo()\n cudaArray* cu_array;\n // Allocate array\n cudaChannelFormatDesc description = cudaCreateChannelDesc();\n cudaMallocArray(&cu_array, &description, width, height);\n // Copy image data to array\n cudaMemcpyToArray(cu_array, image, width*height*sizeof(float), cudaMemcpyHostToDevice);\n // Set texture parameters (default)\n tex.addressMode[0] = cudaAddressModeClamp;\n tex.addressMode[1] = cudaAddressModeClamp;\n tex.filterMode = cudaFilterModePoint;\n tex.normalized = false; // do not normalize coordinates\n // Bind the array to the texture\n cudaBindTextureToArray(tex, cu_array);\n // Run kernel\n dim3 blockDim(16, 16, 1);\n dim3 gridDim((width + blockDim.x - 1)/ blockDim.x, (height + blockDim.y - 1) / blockDim.y, 1);\n kernel\u00ab< gridDim, blockDim, 0 \u00bb>(d_data, height, width);\n // Unbind the array from the texture\n cudaUnbindTexture(tex);\n} //end foo()\n__global__ void kernel(float* odata, int height, int width)\n unsigned int x = blockIdx.x*blockDim.x + threadIdx.x;\n unsigned int y = blockIdx.y*blockDim.y + threadIdx.y;\n if (x < width && y < height) {\n float c = tex2D(tex, x, y);\n odata[y*width+x] = c;\nBelow is an example given in Python that computes the product of two arrays on the GPU. The unofficial Python language bindings can be obtained from \"PyCUDA\".\nimport pycuda.compiler as comp\nimport pycuda.driver as drv\nimport numpy\nimport pycuda.autoinit\nmod = comp.SourceModule(\n__global__ void multiply_them(float *dest, float *a, float *b)\n const int i = threadIdx.x;\n dest[i] = a[i] * b[i];\n\"\"\"\nmultiply_them = mod.get_function(\"multiply_them\")\na = numpy.random.randn(400).astype(numpy.float32)\nb = numpy.random.randn(400).astype(numpy.float32)\ndest = numpy.zeros_like(a)\nmultiply_them(drv.Out(dest), drv.In(a), drv.In(b), block=(400, 1, 1))\nprint(dest - a * b)\nAdditional Python bindings to simplify matrix multiplication operations can be found in the program \"pycublas\".\nimport numpy\nfrom pycublas import CUBLASMatrix\nA = CUBLASMatrix(numpy.mat(1, 2, 3], [4, 5, 6, numpy.float32))\nB = CUBLASMatrix(numpy.mat(2, 3], [4, 5], [6, 7, numpy.float32))\nC = A * B\nprint(C.np_mat())\nwhile CuPy directly replaces NumPy:\nimport cupy\na = cupy.random.randn(400)\nb = cupy.random.randn(400)\ndest = cupy.zeros_like(a)\nprint(dest - a * b)\nGPUs supported.\nSupported CUDA Compute Capability versions for CUDA SDK version and Microarchitecture (by code name):\nNote: CUDA SDK 10.2 is the last official release for macOS, as support will not be available for macOS in newer releases.\nCUDA Compute Capability by version with associated GPU semiconductors and GPU card models (separated by their various application areas):\n'*' \u2013 OEM-only products\nVersion features and specifications.\nData types.\nNote: Any missing lines or empty entries do reflect some lack of information on that exact item.\nTensor cores.\nNote: Any missing lines or empty entries do reflect some lack of information on that exact item.\nMultiprocessor Architecture.\nFor more information read the Nvidia CUDA programming guide.", "categories": ["Category:All Wikipedia articles in need of updating", "Category:All articles with bare URLs for citations", "Category:All articles with unsourced statements", "Category:Articles with GND identifiers", "Category:Articles with J9U identifiers", "Category:Articles with LCCN identifiers", "Category:Articles with PDF format bare URLs for citations", "Category:Articles with bare URLs for citations from August 2022", "Category:Articles with bare URLs for citations from March 2022", "Category:Articles with bare URLs for citations from May 2022"]}
{"docid": 46561507, "title": "Visual Studio Code", "text": "Visual Studio Code, also commonly referred to as VS Code, is a source-code editor made by Microsoft with the Electron Framework, for Windows, Linux and macOS. Features include support for debugging, syntax highlighting, intelligent code completion, snippets, code refactoring, and embedded Git. Users can change the theme, keyboard shortcuts, preferences, and install extensions that add functionality.\nIn the Stack Overflow 2022 Developer Survey, Visual Studio Code was ranked the most popular developer environment tool among 71,010 respondents, with 74.48% reporting that they use it.\nHistory.\nVisual Studio Code was first announced on April 29, 2015, by Microsoft at the 2015 Build conference. A preview build was released shortly thereafter.\nOn November 18, 2015, the source of Visual Studio Code was released under the MIT License, and made available on GitHub. Extension support was also announced. On April 14, 2016, Visual Studio Code graduated from the public preview stage and was released to the Web. Microsoft has released most of Visual Studio Code's source code on GitHub under the permissive MIT License, while the releases by Microsoft are proprietary freeware.\nFeatures.\nVisual Studio Code is a source-code editor that can be used with a variety of programming languages, including C, C#, C++, Fortran, Go, Java, JavaScript, Node.js, Python, Rust, and Julia. It is based on the Electron framework, which is used to develop Node.js web applications that run on the Blink layout engine. Visual Studio Code employs the same editor component (codenamed \"Monaco\") used in Azure DevOps (formerly called Visual Studio Online and Visual Studio Team Services).\nOut of the box, Visual Studio Code includes basic support for most common programming languages. This basic support includes syntax highlighting, bracket matching, code folding, and configurable snippets. Visual Studio Code also ships with IntelliSense for JavaScript, TypeScript, JSON, CSS, and HTML, as well as debugging support for Node.js. Support for additional languages can be provided by freely available extensions on the VS Code Marketplace.\nInstead of a project system, it allows users to open one or more directories, which can then be saved in workspaces for future reuse. This allows it to operate as a language-agnostic code editor for any language. It supports many programming languages and a set of features that differs per language. Unwanted files and folders can be excluded from the project tree via the settings. Many Visual Studio Code features are not exposed through menus or the user interface but can be accessed via the command palette.\nVisual Studio Code can be extended via extensions, available through a central repository. This includes additions to the editor and language support. A notable feature is the ability to create extensions that add support for new languages, themes, debuggers, time travel debuggers, perform static code analysis, and add code linters using the Language Server Protocol.\nSource control is a built-in feature of Visual Studio Code. It has a dedicated tab inside of the menu bar where users can access version control settings and view changes made to the current project. To use the feature, Visual Studio Code must be linked to any supported version control system (Git, Apache Subversion, Perforce, etc.). This allows users to create repositories as well as to make push and pull requests directly from the Visual Studio Code program.\nVisual Studio Code includes multiple extensions for FTP, allowing the software to be used as a free alternative for web development. Code can be synced between the editor and the server, without downloading any extra software.\nVisual Studio Code allows users to set the code page in which the active document is saved, the newline character, and the programming language of the active document. This allows it to be used on any platform, in any locale, and for any given programming language.\nVisual Studio Code collects usage data and sends it to Microsoft, although this can be disabled. Due to the open-source nature of the application, the telemetry code is accessible to the public, who can see exactly what is collected.\nReception.\nIn the 2016 Developers Survey of Stack Overflow, Visual Studio Code ranked No. 13 among the top popular development tools, with only 7% of the 47,000 respondents using it. Two years later, however, Visual Studio Code achieved the No. 1 spot, with 35% of the 75,000 respondents using it. In the 2019 Developers Survey, Visual Studio Code was also ranked No. 1, with 50% of the 87,000 respondents using it. In the 2021 Developers Survey, Visual Studio Code continued to be ranked No. 1, with 74.5% of the 71,000 respondents using it, and 74.48% of the 71,010 responses in the 2022 survey.\nExternal links.\nVSCodium.\nVSCodium is a community-driven, freely-licensed binary distribution of Microsoft's editor VS Code.", "categories": ["Category:2015 software", "Category:All articles with a promotional tone", "Category:Articles with a promotional tone from November 2022", "Category:Articles with short description", "Category:HTML editors", "Category:Java development tools", "Category:Linux text editors", "Category:MacOS text editors", "Category:Microsoft Visual Studio", "Category:Microsoft free software"]}
{"docid": 324378, "title": "C standard library", "text": "The C standard library or libc is the standard library for the C programming language, as specified in the ISO C standard. Starting from the original ANSI C standard, it was developed at the same time as the C library POSIX specification, which is a superset of it. Since ANSI C was adopted by the International Organization for Standardization, the C standard library is also called the ISO C library.\nThe C standard library provides macros, type definitions and functions for tasks such as string handling, mathematical computations, input/output processing, memory management, and several other operating system services.\nApplication programming interface (API).\nHeader files.\nThe application programming interface (API) of the C standard library is declared in a number of header files. Each header file contains one or more function declarations, data type definitions, and macros.\nAfter a long period of stability, three new header files (codice_1, codice_2, and codice_3) were added with \"Normative Addendum 1\" (NA1), an addition to the C Standard ratified in 1995. Six more header files (codice_4, codice_5, codice_6, codice_7, codice_8, and codice_9) were added with C99, a revision to the C Standard published in 1999, and five more files (codice_10, codice_11, codice_12, codice_13, and codice_14) with C11 in 2011. In total, there are now 29 header files:\nThree of the header files (codice_4, codice_11, and codice_13) are conditional features that implementations are not required to support.\nThe POSIX standard added several nonstandard C headers for Unix-specific functionality. Many have found their way to other architectures. Examples include codice_18 and codice_19. A number of other groups are using other nonstandard headers \u2013 the GNU C Library has codice_20, and HP OpenVMS has the codice_21 function.\nDocumentation.\nOn Unix-like systems, the authoritative documentation of the actually implemented API is provided in the form of man pages. On most systems, man pages on standard library functions are in section\u00a03; section\u00a07 may contain some more generic pages on underlying concepts (e.g. codice_22 in Linux).\nImplementations.\nUnix-like systems typically have a C library in shared library form, but the header files (and compiler toolchain) may be absent from an installation so C development may not be possible. The C library is considered part of the operating system on Unix-like systems. The C functions, including the ISO C standard ones, are widely used by programs, and are regarded as if they were not only an implementation of something in the C language, but also \"de facto\" part of the operating system interface. Unix-like operating systems generally cannot function if the C library is erased. This is true for applications which are dynamically as opposed to statically linked. Further, the kernel itself (at least in the case of Linux) operates independently of any libraries. \nOn Microsoft Windows, the core system dynamic libraries (DLLs) provide an implementation of the C standard library for the Microsoft Visual\u00a0C++ compiler v6.0; the C standard library for newer versions of the Microsoft Visual\u00a0C++ compiler is provided by each compiler individually, as well as \"redistributable\" packages. Compiled applications written in C are either statically linked with a C library, or linked to a dynamic version of the library that is shipped with these applications, rather than relied upon to be present on the targeted systems. Functions in a compiler's C library are not regarded as interfaces to Microsoft Windows.\nMany other implementations exist, provided with both various operating systems and C compilers. Some of the popular implementations are the following:\nCompiler built-in functions.\nSome compilers (for example, GCC) provide built-in versions of many of the functions in the C standard library; that is, the implementations of the functions are written into the compiled object file, and the program calls the built-in versions instead of the functions in the C library shared object file. This reduces function-call overhead, especially if function calls are replaced with inline variants, and allows other forms of optimization (as the compiler knows the control-flow characteristics of the built-in variants), but may cause confusion when debugging (for example, the built-in versions cannot be replaced with instrumented variants).\nHowever, the built-in functions must behave like ordinary functions in accordance with ISO C. The main implication is that the program must be able to create a pointer to these functions by taking their address, and invoke the function by means of that pointer. If two pointers to the same function are derived in two different translation units in the program, these two pointers must compare equal; that is, the address comes by resolving the name of the function, which has external (program-wide) linkage.\nLinking, libm.\nUnder FreeBSD and glibc, some functions such as sin() are not linked in by default and are instead bundled in the mathematical library libm. If any of them are used, the linker must be given the directive codice_23. POSIX requires that the c99 compiler supports codice_23, and that the functions declared in the headers codice_25, codice_4, and codice_5 are available for linking if codice_23 is specified, but does not specify if the functions are linked by default. musl satisfies this requirement by putting everything into a single libc library and providing an empty libm.\nDetection.\nAccording to the C standard the macro codice_29 shall be defined to 1 if the implementation is hosted. A hosted implementation has all the headers specified by the C standard. An implementation can also be \"freestanding\" which means that these headers will not be present. If an implementation is \"freestanding\", it shall define codice_29 to 0.\nProblems and workarounds.\nBuffer overflow vulnerabilities.\nSome functions in the C standard library have been notorious for having buffer overflow vulnerabilities and generally encouraging buggy programming ever since their adoption. The most criticized items are:\nExcept the extreme case with codice_34, all the security vulnerabilities can be avoided by introducing auxiliary code to perform memory management, bounds checking, input checking, etc. This is often done in the form of wrappers that make standard library functions safer and easier to use. This dates back to as early as \"The Practice of Programming\" book by B. Kernighan and R. Pike where the authors commonly use wrappers that print error messages and quit the program if an error occurs.\nThe ISO C committee published Technical reports TR 24731-1 and is working on TR 24731-2 to propose adoption of some functions with bounds checking and automatic buffer allocation, correspondingly. The former has met severe criticism with some praise, the latter received mixed responses. Despite this, TR 24731-1 has been implemented into Microsoft's C standard library and its compiler issues warnings when using old \"insecure\" functions.\nThreading problems, vulnerability to race conditions.\nThe codice_37 routine is criticized for being thread unsafe and otherwise vulnerable to race conditions.\nError handling.\nThe error handling of the functions in the C standard library is not consistent and sometimes confusing. According to the Linux manual page codice_38, \"The current (version 2.8) situation under glibc is messy. Most (but not all) functions raise exceptions on errors. Some also set \"errno\". A few functions set \"errno\", but don't raise an exception. A very few functions do neither.\"\nStandardization.\nThe original C language provided no built-in functions such as I/O operations, unlike traditional languages such as COBOL and Fortran. Over time, user communities of C shared ideas and implementations of what is now called C standard libraries. Many of these ideas were incorporated eventually into the definition of the standardized C language.\nBoth Unix and C were created at AT&T's Bell Laboratories in the late 1960s and early 1970s. During the 1970s the C language became increasingly popular. Many universities and organizations began creating their own variants of the language for their own projects. By the beginning of the 1980s compatibility problems between the various C implementations became apparent. In 1983 the American National Standards Institute (ANSI) formed a committee to establish a standard specification of C known as \"ANSI C\". This work culminated in the creation of the so-called C89 standard in 1989. Part of the resulting standard was a set of software libraries called the ANSI C standard library.\nPOSIX standard library.\nPOSIX, as well as SUS, specify a number of routines that should be available over and above those in the basic C standard library. The POSIX specification includes header files for, among other uses, multi-threading, networking, and regular expressions. These are often implemented alongside the C standard library functionality, with varying degrees of closeness. For example, glibc implements functions such as codice_39 within codice_40, but before NPTL was merged into glibc it constituted a separate library with its own linker flag argument. Often, this POSIX-specified functionality will be regarded as part of the library; the basic C library may be identified as the ANSI or ISO C library.\nBSD libc.\nBSD libc is a superset of the POSIX standard library supported by the C libraries included with BSD operating systems such as FreeBSD, NetBSD, OpenBSD and macOS. BSD libc has some extensions that are not defined in the original standard, many of which first appeared in 1994's 4.4BSD release (the first to be largely developed after the first standard was issued in 1989). Some of the extensions of BSD libc are:\nThe C standard library in other languages.\nSome languages include the functionality of the standard C library in their own libraries. The library may be adapted to better suit the language's structure, but the operational semantics are kept similar. The C++ language, for example, includes the functionality of the C standard library in the namespace codice_54 (e.g., codice_55, codice_56, codice_57), in header files with similar names to the C ones (codice_58, codice_59, codice_60, etc.). Other languages that take similar approaches are D, Perl, Ruby and the main implementation of Python known as CPython. In Python 2, for example, the built-in file objects are defined as \"implemented using C's codice_61 package\", so that the available operations (open, read, write, etc.) are expected to have the same behavior as the corresponding C functions. Rust has a crate called which allows several C functions, structs, and other type definitions to be used.\nComparison to standard libraries of other languages.\nThe C standard library is small compared to the standard libraries of some other languages. The C library provides a basic set of mathematical functions, string manipulation, type conversions, and file and console-based I/O. It does not include a standard set of \"container types\" like the C++ Standard Template Library, let alone the complete graphical user interface (GUI) toolkits, networking tools, and profusion of other functionality that Java and the .NET Framework provide as standard. The main advantage of the small standard library is that providing a working ISO C environment is much easier than it is with other languages, and consequently porting C to a new platform is comparatively easy.", "categories": ["Category:All articles with unsourced statements", "Category:Articles with short description", "Category:Articles with unsourced statements from November 2010", "Category:CS1 maint: url-status", "Category:C (programming language)", "Category:C standard library", "Category:Short description is different from Wikidata", "Category:Webarchive template wayback links"]}
{"docid": 93817, "title": "Data type", "text": "In computer science and computer programming, a data type (or simply type) is a collection or grouping of data values, usually specified by a set of possible values, a set of allowed operations on these values, and/or a representation of these values as machine types. A data type specification in a program constrains the possible values that an expression, such as a variable or a function call, might take. On literal data, it tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans.\nConcept.\nA data type may be specified for many reasons: similarity, convenience, or to focus the attention. It is frequently a matter of good organization\nthat aids the understanding of complex definitions. Almost all programming languages explicitly include the notion of data type, though the possible data types are often restricted by considerations of simplicity, computability, or regularity. An explicit data type declaration typically allows the compiler to choose an efficient machine representation, but the conceptual organization offered by data types should not be discounted.\nDifferent languages may use different data types or similar types with different semantics. For example, in the Python programming language, codice_1 represents an arbitrary-precision integer which has the traditional numeric operations such as addition, subtraction, and multiplication. However, in the Java programming language, the type codice_1 represents the set of 32-bit integers ranging in value from \u22122,147,483,648 to 2,147,483,647, with arithmetic operations that wrap on overflow. In Rust this 32-bit integer type is denoted codice_3 and panics on overflow in debug mode.\nMost programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type. For example, a programmer might create a new data type named \"complex number\" that would include real and imaginary parts, or a color data type represented by three bytes denoting the amounts each of red, green, and blue, and a string representing the color's name.\nData types are used within type systems, which offer various ways of defining, implementing, and using them. In a type system, a data type represents a constraint placed upon the interpretation of data, describing representation, interpretation and structure of values or objects stored in computer memory. The type system uses data type information to check correctness of computer programs that access or manipulate the data. A compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).\nMost data types in statistics have comparable types in computer programming, and vice versa, as shown in the following table:\nDefinition.\n identified five definitions of a \"type\" that were used\u2014sometimes implicitly\u2014in the literature:\nThe definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU. Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.\nClassification.\nData types may be categorized according to several factors:\nThe terminology varies - in the literature, primitive, built-in, basic, atomic, and fundamental may be used interchangeably.\nExamples.\nMachine data types.\nAll data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level. The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits). The unit processed by machine code instructions is called a word (, typically 32 or 64 bits).\nMachine data types \"expose\" or make available fine-grained control over hardware, but this can also expose implementation details that make code less portable. Hence machine types are mainly used in systems programming or low-level programming languages. In higher-level languages most data types are \"abstracted\" in that they do not have a language-defined machine representation. The C programming language, for instance, supplies types such as booleans, integers, floating-point numbers, etc., but the precise bit representations of these types are implementation-defined. The only C type with a precise machine representation is the codice_4 type that represents a byte.\nBoolean type.\nThe Boolean type represents the values true and false. Although only two values are possible, they are more often represented as a word rather as a single bit as it requires more machine instructions to store and retrieve an individual bit. Many programming languages do not have an explicit Boolean type, instead using an integer type and interpreting (for instance) 0 as false and other values as true.\nBoolean data refers to the logical structure of how the language is interpreted to the machine language. In this case a Boolean 0 refers to the logic False. True is always a non zero, especially a one which is known as Boolean 1.\nNumeric types.\nAlmost all programming languages supply one or more integer data types. They may either supply a small number of predefined subtypes restricted to certain ranges (such as codice_5 and codice_6 and their corresponding codice_7 variants in C/C++); or allow users to freely define subranges such as 1..12 (e.g. Pascal/Ada). If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.\nFloating point data types represent certain fractional values (rational numbers, mathematically). Although they have predefined limits on both their maximum values and their precision, they are sometimes misleadingly called reals (evocative of mathematical real numbers). They are typically stored internally in the form (where and are integers), but displayed in familiar decimal form.\nFixed point data types are convenient for representing monetary values. They are often implemented internally as integers, leading to predefined limits.\nFor independence from architecture details, a Bignum or arbitrary precision codice_8 type might be supplied. This represents an integer or rational to a precision limited only by the available memory and computational resources on the system. Bignum implementations of arithmetic operations on machine-sized values are significantly slower than the corresponding machine operations.\nEnumerations.\nThe enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer's memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named \"CLUB\", \"DIAMOND\", \"HEART\", \"SPADE\", belonging to an enumerated type named \"suit\". If a variable \"V\" is declared having \"suit\" as its data type, one can assign any of those four values to it. Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers.\nString and text types.\nStrings are a sequence of characters used to store words or plain text, most often textual markup languages representing formatted text. Characters may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc. Characters are drawn from a character set such as ASCII. Character and string types can have different subtypes according to the character encoding. The original 7-bit wide ASCII was found to be limited, and superseded by 8, 16 and 32-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols. Strings may be of either variable length or fixed length, and some programming languages have both types. They may also be subtyped by their maximum size.\nSince most character sets include the digits, it is possible to have a numeric string, such as codice_9. These numeric strings are usually considered distinct from numeric values such as codice_10, although some languages automatically convert between them.\nUnion types.\nA union type definition will specify which of a number of permitted subtypes may be stored in its instances, e.g. \"float or long integer\". In contrast with a record, which could be defined to contain a float \"and\" an integer, a union may only contain one subtype at a time.\nA tagged union (also called a variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type for enhanced type safety.\nAlgebraic data types.\nAn algebraic data type (ADT) is a possibly recursive sum type of product types. A value of an ADT consists of a constructor tag together with zero or more field values, with the number and type of the field values fixed by the constructor. The set of all possible values of an ADT is the set-theoretic disjoint union (sum), of the sets of all possible values of its variants (product of fields). Values of algebraic types are analyzed with pattern matching, which identifies a value's constructor and extracts the fields it contains.\nIf there is only one constructor, then the ADT corresponds to a product type similar to a tuple or record. A constructor with no fields corresponds to the empty product (unit type). If all constructors have no fields then the ADT corresponds to an enumerated type.\nOne common ADT is the option type, defined in Haskell as .\nData structures.\nSome types are very useful for storing and retrieving data and are called data structures. Common data structures include:\nAbstract data types.\nAn abstract data type is a data type that does not specify the concrete representation of the data. Instead, a formal \"specification\" based on the data type's operations is used to describe it. Any \"implementation\" of a specification must fulfill the rules given. For example, a stack has push/pop operations that follow a Last-In-First-Out rule, and can be concretely implemented using either a list or an array. Another example is a set which stores values, without any particular order, and no repeated values. Values themselves are not retrieved from sets, rather one tests a value for membership to obtain a boolean \"in\" or \"not in\".\nAbstract data types are used in formal semantics and program verification and, less strictly, in design. Beyond verification, a specification might immediately be turned into an implementation. The OBJ family of programming languages for instance bases on this option using equations for specification and rewriting to run them. Algebraic specification was an important subject of research in CS around 1980 and almost a synonym for abstract data types at that time. It has a mathematical foundation in universal algebra. The specification language can be made more expressive by allowing other formulas than only equations.\nA more involved example is the Boom hierarchy of the binary tree, list, bag and set abstract data types. All these data types can be declared by three operations: \"null\", which constructs the empty container, \"single\", which constructs a container from a single element and \"append\", which combines two containers of the same type. The complete specification for the four data types can then be given by successively adding the following rules over these operations:\nAccess to the data can be specified by pattern-matching over the three operations, e.g. a \"member\" function for these containers by:\nCare must be taken to ensure that the function is invariant under the relevant rules for the data type. Within each of the equivalence classes implied by the chosen subset of equations, it has to yield the same result for all of its members.\nPointers and references.\nThe main non-composite, derived type is the pointer, a data type whose value refers directly to (or \"points to\") another value stored elsewhere in the computer memory using its address. It is a primitive kind of reference. (In everyday terms, a page number in a book could be considered a piece of data that refers to another one). Pointers are often stored in a format similar to an integer; however, attempting to dereference or \"look up\" a pointer whose value was never a valid memory address would cause a program to crash. To ameliorate this potential problem, pointers are considered a separate type to the type of data they point to, even if the underlying representation is the same.\nFunction types.\nFunctional programming languages treat functions as a distinct datatype and allow values of this type to be stored in variables and passed to functions. Some multi-paradigm languages such as JavaScript also have mechanisms for treating functions as data. Most contemporary type systems go beyond JavaScript's simple type \"function object\" and have a family of function types differentiated by argument and return types, such as the type codice_11 denoting functions taking an integer and returning a boolean. In C, a function is not a first-class data type but function pointers can be manipulated by the program. Java and C++ originally did not have function values but have added them in C++11 and Java 8.\nType constructors.\nA type constructor builds new types from old ones, and can be thought of as an operator taking zero or more types as arguments and producing a type. Product types, function types, power types and list types can be made into type constructors.\nQuantified types.\nUniversally-quantified and existentially-quantified types are based on predicate logic. Universal quantification is written as formula_1 or codice_12 and is the intersection over all types codice_13 of the body codice_14, i.e. the value is of type codice_14 for every codice_13. Existential quantification written as formula_2 or codice_17 and is the union over all types codice_13 of the body codice_14, i.e. the value is of type codice_14 for some codice_13.\nIn Haskell, universal quantification is commonly used, but existential types must be encoded by transforming codice_22 to codice_23 or a similar type.\nRefinement types.\nA refinement type is a type endowed with a predicate which is assumed to hold for any element of the refined type. For instance, the type of natural numbers greater than 5 may be written as formula_3\nDependent types.\nA dependent type is a type whose definition depends on a value. Two common examples of dependent types are dependent functions and dependent pairs. The return type of a dependent function may depend on the value (not just type) of one of its arguments. A dependent pair may have a second value of which the type depends on the first value.\nIntersection types.\nAn intersection type is a type containing those values that are members of two specified types. For example, in Java the class implements both the and the interfaces. Therefore, an object of type is a member of the type . Considering types as sets of values, the intersection type formula_4 is the set-theoretic intersection of formula_5 and formula_6. It is also possible to define a dependent intersection type, denoted formula_7, where the type formula_6 may depend on the term variable formula_9.\nMeta types.\nSome programming languages represent the type information as data, enabling type introspection and reflection. In contrast, higher order type systems, while allowing types to be constructed from other types and passed to functions as values, typically avoid basing computational decisions on them.\nConvenience types.\nFor convenience, high-level languages and databases may supply ready-made \"real world\" data types, for instance times, dates, and monetary values (currency). These may be built-in to the language or implemented as composite types in a library.", "categories": ["Category:All articles containing potentially dated statements", "Category:All articles needing additional references", "Category:All articles with unsourced statements", "Category:Articles containing potentially dated statements from 2011", "Category:Articles needing additional references from June 2021", "Category:Articles with GND identifiers", "Category:Articles with J9U identifiers", "Category:Articles with LCCN identifiers", "Category:Articles with LNB identifiers", "Category:Articles with NKC identifiers"]}
{"docid": 6021, "title": "C (programming language)", "text": "C (\"pronounced\" \" \u2013 like the letter c\") is a general-purpose computer programming language. It was created in the 1970s by Dennis Ritchie, and remains very widely used and influential. By design, C's features cleanly reflect the capabilities of the targeted CPUs. It has found lasting use in operating systems, device drivers, protocol stacks, though decreasingly for application software. C is commonly used on computer architectures that range from the largest supercomputers to the smallest microcontrollers and embedded systems.\nA successor to the programming language B, C was originally developed at Bell Labs by Ritchie between 1972 and 1973 to construct utilities running on Unix. It was applied to re-implementing the kernel of the Unix operating system. During the 1980s, C gradually gained popularity. It has become one of the most widely used programming languages, with C compilers available for practically all modern computer architectures and operating systems. The book \"The C Programming Language\", co-authored by the original language designer, served for many years as the \"de facto\" standard for the language.\nC has been standardized by ANSI since 1989 (ANSI C) and by the International Organization for Standardization (ISO).\nC is an imperative procedural language, supporting structured programming, lexical variable scope and recursion, with a static type system. It was designed to be compiled to provide low-level access to memory and language constructs that map efficiently to machine instructions, all with minimal runtime support. Despite its low-level capabilities, the language was designed to encourage cross-platform programming. A standards-compliant C program written with portability in mind can be compiled for a wide variety of computer platforms and operating systems with few changes to its source code.\nSince 2000, C has consistently ranked among the top two languages in the TIOBE index, a measure of the popularity of programming languages.\nOverview.\nC is an imperative, procedural language in the ALGOL tradition. It has a static type system. In C, all executable code is contained within subroutines (also called \"functions\", though not in the sense of functional programming). Function parameters are passed by value, although arrays are passed as pointers, i.e. the address of the first item in the array. \"Pass-by-reference\" is simulated in C by explicitly passing pointers to the thing being referenced.\nC program source text is free-form code. The semicolon separates statements and curly braces are used for grouping blocks of statements.\nThe C language also exhibits the following characteristics:\nWhile C does not include certain features found in other languages (such as object orientation and garbage collection), these can be implemented or emulated, often through the use of external libraries (e.g., the GLib Object System or the Boehm garbage collector).\nRelations to other languages.\nMany later languages have borrowed directly or indirectly from C, including C++, C#, Unix's C shell, D, Go, Java, JavaScript (including transpilers), Julia, Limbo, LPC, Objective-C, Perl, PHP, Python, Ruby, Rust, Swift, Verilog and SystemVerilog (hardware description languages). These languages have drawn many of their control structures and other basic features from C. Most of them (Python being a dramatic exception) also express highly similar syntax to C, and they tend to combine the recognizable expression and statement syntax of C with underlying type systems, data models, and semantics that can be radically different.\nHistory.\nEarly developments.\nThe origin of C is closely tied to the development of the Unix operating system, originally implemented in assembly language on a PDP-7 by Dennis Ritchie and Ken Thompson, incorporating several ideas from colleagues. Eventually, they decided to port the operating system to a PDP-11. The original PDP-11 version of Unix was also developed in assembly language.\nB.\nThompson wanted a programming language for developing utilities for the new platform. At first, he tried to write a Fortran compiler, but soon gave up the idea. Instead, he created a cut-down version of the recently developed BCPL systems programming language. The official description of BCPL was not available at the time and Thompson modified the syntax to be less wordy, and similar to a simplified ALGOL known as SMALGOL. Thompson called the result \"B\". He described B as \"BCPL semantics with a lot of SMALGOL syntax\". Like BCPL, B had a bootstrapping compiler to facilitate porting to new machines. However, few utilities were ultimately written in B because it was too slow, and could not take advantage of PDP-11 features such as byte addressability.\nNew B and first C release.\nIn 1971, Ritchie started to improve B, to utilise the features of the more-powerful PDP-11. A significant addition was a character data type. He called this \"New B\" (NB). Thompson started to use NB to write the Unix kernel, and his requirements shaped the direction of the language development. Through to 1972, richer types were added to the NB language: NB had arrays of codice_12 and codice_13. Pointers, the ability to generate pointers to other types, arrays of all types, and types to be returned from functions were all also added. Arrays within expressions became pointers. A new compiler was written, and the language was renamed C.\nThe C compiler and some utilities made with it were included in Version 2 Unix, which is also known as Research Unix.\nStructures and the Unix kernel re-write.\nAt Version 4 Unix, released in November 1973, the Unix kernel was extensively re-implemented in C. By this time, the C language had acquired some powerful features such as codice_6 types.\nThe preprocessor was introduced around 1973 at the urging of Alan Snyder and also in recognition of the usefulness of the file-inclusion mechanisms available in BCPL and PL/I. Its original version provided only included files and simple string replacements: codice_15 and codice_16 of parameterless macros. Soon after that, it was extended, mostly by Mike Lesk and then by John Reiser, to incorporate macros with arguments and conditional compilation.\nUnix was one of the first operating system kernels implemented in a language other than assembly. Earlier instances include the Multics system (which was written in PL/I) and Master Control Program (MCP) for the Burroughs B5000 (which was written in ALGOL) in 1961. In around 1977, Ritchie and Stephen C. Johnson made further changes to the language to facilitate portability of the Unix operating system. Johnson's Portable C Compiler served as the basis for several implementations of C on new platforms.\nK&R C.\nIn 1978, Brian Kernighan and Dennis Ritchie published the first edition of \"The C Programming Language\". This book, known to C programmers as \"K&R\", served for many years as an informal specification of the language. The version of C that it describes is commonly referred to as \"K&R C\". As this was released in 1978, it is also referred to as \"C78\". The second edition of the book covers the later ANSI C standard, described below.\n\"K&R\" introduced several language features:\nEven after the publication of the 1989 ANSI standard, for many years K&R C was still considered the \"lowest common denominator\" to which C programmers restricted themselves when maximum portability was desired, since many older compilers were still in use, and because carefully written K&R C code can be legal Standard C as well.\nIn early versions of C, only functions that return types other than codice_12 must be declared if used before the function definition; functions used without prior declaration were presumed to return type codice_12.\nFor example:\nlong some_function(); /* This is a function declaration, so the compiler can know the name and return type of this function. */\n/* int */ other_function(); /* Another function declaration. Because this is an early version of C, there is an implicit 'int' type here. A comment shows where the explicit 'int' type specifier would be required in later versions. */\n/* int */ calling_function() /* This is a function definition, including the body of the code following in the { curly brackets }. Because no return type is specified, the function implicitly returns an 'int' in this early version of C. */\n long test1;\n register /* int */ test2; /* Again, note that 'int' is not required here. The 'int' type specifier */\n /* in the comment would be required in later versions of C. */\n /* The 'register' keyword indicates to the compiler that this variable should */\n /* ideally be stored in a register as opposed to within the stack frame. */\n test1 = some_function();\n if (test1 > 1)\n test2 = 0;\n else\n test2 = other_function();\n return test2;\nThe codice_12 type specifiers which are commented out could be omitted in K&R C, but are required in later standards.\nSince K&R function declarations did not include any information about function arguments, function parameter type checks were not performed, although some compilers would issue a warning message if a local function was called with the wrong number of arguments, or if multiple calls to an external function used different numbers or types of arguments. Separate tools such as Unix's lint utility were developed that (among other things) could check for consistency of function use across multiple source files.\nIn the years following the publication of K&R C, several features were added to the language, supported by compilers from AT&T (in particular PCC) and some other vendors. These included:\nThe large number of extensions and lack of agreement on a standard library, together with the language popularity and the fact that not even the Unix compilers precisely implemented the K&R specification, led to the necessity of standardization.\nANSI C and ISO C.\nDuring the late 1970s and 1980s, versions of C were implemented for a wide variety of mainframe computers, minicomputers, and microcomputers, including the IBM PC, as its popularity began to increase significantly.\nIn 1983, the American National Standards Institute (ANSI) formed a committee, X3J11, to establish a standard specification of C. X3J11 based the C standard on the Unix implementation; however, the non-portable portion of the Unix C library was handed off to the IEEE working group 1003 to become the basis for the 1988 POSIX standard. In 1989, the C standard was ratified as ANSI X3.159-1989 \"Programming Language C\". This version of the language is often referred to as ANSI C, Standard C, or sometimes C89.\nIn 1990, the ANSI C standard (with formatting changes) was adopted by the International Organization for Standardization (ISO) as ISO/IEC 9899:1990, which is sometimes called C90. Therefore, the terms \"C89\" and \"C90\" refer to the same programming language.\nANSI, like other national standards bodies, no longer develops the C standard independently, but defers to the international C standard, maintained by the working group ISO/IEC JTC1/SC22/WG14. National adoption of an update to the international standard typically occurs within a year of ISO publication.\nOne of the aims of the C standardization process was to produce a superset of K&R C, incorporating many of the subsequently introduced unofficial features. The standards committee also included several additional features such as function prototypes (borrowed from C++), codice_9 pointers, support for international character sets and locales, and preprocessor enhancements. Although the syntax for parameter declarations was augmented to include the style used in C++, the K&R interface continued to be permitted, for compatibility with existing source code.\nC89 is supported by current C compilers, and most modern C code is based on it. Any program written only in Standard C and without any hardware-dependent assumptions will run correctly on any platform with a conforming C implementation, within its resource limits. Without such precautions, programs may compile only on a certain platform or with a particular compiler, due, for example, to the use of non-standard libraries, such as GUI libraries, or to a reliance on compiler- or platform-specific attributes such as the exact size of data types and byte endianness.\nIn cases where code must be compilable by either standard-conforming or K&R C-based compilers, the codice_37 macro can be used to split the code into Standard and K&R sections to prevent the use on a K&R C-based compiler of features available only in Standard C.\nAfter the ANSI/ISO standardization process, the C language specification remained relatively static for several years. In 1995, Normative Amendment 1 to the 1990 C standard (ISO/IEC 9899/AMD1:1995, known informally as C95) was published, to correct some details and to add more extensive support for international character sets.\nC99.\nThe C standard was further revised in the late 1990s, leading to the publication of ISO/IEC 9899:1999 in 1999, which is commonly referred to as \"C99\". It has since been amended three times by Technical Corrigenda.\nC99 introduced several new features, including inline functions, several new data types (including codice_38 and a codice_39 type to represent complex numbers), variable-length arrays and flexible array members, improved support for IEEE 754 floating point, support for variadic macros (macros of variable arity), and support for one-line comments beginning with codice_40, as in BCPL or C++. Many of these had already been implemented as extensions in several C compilers.\nC99 is for the most part backward compatible with C90, but is stricter in some ways; in particular, a declaration that lacks a type specifier no longer has codice_12 implicitly assumed. A standard macro codice_42 is defined with value codice_43 to indicate that C99 support is available. GCC, Solaris Studio, and other C compilers now support many or all of the new features of C99. The C compiler in Microsoft Visual C++, however, implements the C89 standard and those parts of C99 that are required for compatibility with C++11.\nIn addition, the C99 standard requires support for Unicode identifiers in the form of escaped characters (e.g. or ) and suggests support for raw Unicode names.\nC11.\nIn 2007, work began on another revision of the C standard, informally called \"C1X\" until its official publication of ISO/IEC 9899:2011 on 2011-12-08. The C standards committee adopted guidelines to limit the adoption of new features that had not been tested by existing implementations.\nThe C11 standard adds numerous new features to C and the library, including type generic macros, anonymous structures, improved Unicode support, atomic operations, multi-threading, and bounds-checked functions. It also makes some portions of the existing C99 library optional, and improves compatibility with C++. The standard macro codice_42 is defined as codice_45 to indicate that C11 support is available.\nC17.\nPublished in June 2018 as ISO/IEC 9899:2018, C17 is the current standard for the C programming language. It introduces no new language features, only technical corrections, and clarifications to defects in C11. The standard macro codice_42 is defined as codice_47.\nC23.\nC23 is the informal name for the next (after C17) major C language standard revision. It is expected to be published in 2024.\nEmbedded C.\nHistorically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\nIn 2008, the C Standards Committee published a technical report extending the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces, and basic I/O hardware addressing.\nSyntax.\nC has a formal grammar specified by the C standard. Line endings are generally not significant in C; however, line boundaries do have significance during the preprocessing phase. Comments may appear either between the delimiters codice_48 and codice_49, or (since C99) following codice_40 until the end of the line. Comments delimited by codice_48 and codice_49 do not nest, and these sequences of characters are not interpreted as comment delimiters if they appear inside string or character literals.\nC source files contain declarations and function definitions. Function definitions, in turn, contain declarations and statements. Declarations either define new types using keywords such as codice_6, codice_33, and codice_8, or assign types to and perhaps reserve storage for new variables, usually by writing the type followed by the variable name. Keywords such as codice_13 and codice_12 specify built-in types. Sections of code are enclosed in braces (codice_58 and codice_59, sometimes called \"curly brackets\") to limit the scope of declarations and to act as a single statement for control structures.\nAs an imperative language, C uses \"statements\" to specify actions. The most common statement is an \"expression statement\", consisting of an expression to be evaluated, followed by a semicolon; as a side effect of the evaluation, functions may be called and variables may be assigned new values. To modify the normal sequential execution of statements, C provides several control-flow statements identified by reserved keywords. Structured programming is supported by codice_60 ... [codice_61] conditional execution and by codice_62 ... codice_4, codice_4, and codice_2 iterative execution (looping). The codice_2 statement has separate initialization, testing, and reinitialization expressions, any or all of which can be omitted. codice_67 and codice_68 can be used within the loop. Break is used to leave the innermost enclosing loop statement and continue is used to skip to its reinitialisation. There is also a non-structured codice_69 statement which branches directly to the designated label within the function. codice_5 selects a codice_71 to be executed based on the value of an integer expression. Different from many other languages, control-flow will fall through to the next codice_71 unless terminated by a codice_67.\nExpressions can use a variety of built-in operators and may contain function calls. The order in which arguments to functions and operands to most operators are evaluated is unspecified. The evaluations may even be interleaved. However, all side effects (including storage to variables) will occur before the next \"sequence point\"; sequence points include the end of each expression statement, and the entry to and return from each function call. Sequence points also occur during evaluation of expressions containing certain operators (codice_74, codice_75, codice_76 and the comma operator). This permits a high degree of object code optimization by the compiler, but requires C programmers to take more care to obtain reliable results than is needed for other programming languages.\nKernighan and Ritchie say in the Introduction of \"The C Programming Language\": \"C, like any other language, has its blemishes. Some of the operators have the wrong precedence; some parts of the syntax could be better.\" The C standard did not attempt to correct many of these blemishes, because of the impact of such changes on already existing software.\nCharacter set.\nThe basic C source character set includes the following characters:\nNewline indicates the end of a text line; it need not correspond to an actual single character, although for convenience C treats it as one.\nAdditional multi-byte encoded characters may be used in string literals, but they are not entirely portable. The latest C standard (C11) allows multi-national Unicode characters to be embedded portably within C source text by using codice_84 or codice_85 encoding (where the codice_86 denotes a hexadecimal character), although this feature is not yet widely implemented.\nThe basic C execution character set contains the same characters, along with representations for alert, backspace, and carriage return. Run-time support for extended character sets has increased with each revision of the C standard.\nReserved words.\nC89 has 32 reserved words, also known as keywords, which are the words that cannot be used for any purposes other than those for which they are predefined:\nC99 reserved five more words:\nC11 reserved seven more words:\nMost of the recently reserved words begin with an underscore followed by a capital letter, because identifiers of that form were previously reserved by the C standard for use only by implementations. Since existing program source code should not have been using these identifiers, it would not be affected when C implementations started supporting these extensions to the programming language. Some standard headers do define more convenient synonyms for underscored identifiers. The language previously included a reserved word called codice_131, but this was seldom implemented, and has now been removed as a reserved word.\nOperators.\nC supports a rich set of operators, which are symbols used within an expression to specify the manipulations to be performed while evaluating that expression. C has operators for:\nC uses the operator codice_137 (used in mathematics to express equality) to indicate assignment, following the precedent of Fortran and PL/I, but unlike ALGOL and its derivatives. C uses the operator codice_147 to test for equality. The similarity between these two operators (assignment and equality) may result in the accidental use of one in place of the other, and in many cases, the mistake does not produce an error message (although some compilers produce warnings). For example, the conditional expression codice_167 might mistakenly be written as codice_168, which will be evaluated as true if codice_77 is not zero after the assignment.\nThe C operator precedence is not always intuitive. For example, the operator codice_147 binds more tightly than (is executed prior to) the operators codice_139 (bitwise AND) and codice_140 (bitwise OR) in expressions such as codice_173, which must be written as codice_174 if that is the coder's intent.\n\"Hello, world\" example.\nThe \"hello, world\" example, which appeared in the first edition of \"K&R\", has become the model for an introductory program in most programming textbooks. The program prints \"hello, world\" to the standard output, which is usually a terminal or screen display.\nThe original version was:\nmain()\n printf(\"hello, world\\n\");\nA standard-conforming \"hello, world\" program is:\nint main(void)\n printf(\"hello, world\\n\");\nThe first line of the program contains a preprocessing directive, indicated by codice_15. This causes the compiler to replace that line with the entire text of the codice_176 standard header, which contains declarations for standard input and output functions such as codice_177 and codice_178. The angle brackets surrounding codice_176 indicate that codice_176 can be located using a search strategy that prefers headers provided with the compiler to other headers having the same name, as opposed to double quotes which typically include local or project-specific header files.\nThe next line indicates that a function named codice_181 is being defined. The codice_181 function serves a special purpose in C programs; the run-time environment calls the codice_181 function to begin program execution. The type specifier codice_12 indicates that the value that is returned to the invoker (in this case the run-time environment) as a result of evaluating the codice_181 function, is an integer. The keyword codice_9 as a parameter list indicates that this function takes no arguments.\nThe opening curly brace indicates the beginning of the definition of the codice_181 function.\nThe next line \"calls\" (diverts execution to) a function named codice_177, which in this case is supplied from a system library. In this call, the codice_177 function is \"passed\" (provided with) a single argument, the address of the first character in the string literal codice_190. The string literal is an unnamed array with elements of type codice_13, set up automatically by the compiler with a final 0-valued character to mark the end of the array (codice_177 needs to know this). The codice_193 is an \"escape sequence\" that C translates to a \"newline\" character, which on output signifies the end of the current line. The return value of the codice_177 function is of type codice_12, but it is silently discarded since it is not used. (A more careful program might test the return value to determine whether or not the codice_177 function succeeded.) The semicolon codice_197 terminates the statement.\nThe closing curly brace indicates the end of the code for the codice_181 function. According to the C99 specification and newer, the codice_181 function, unlike any other function, will implicitly return a value of codice_81 upon reaching the codice_59 that terminates the function. (Formerly an explicit codice_202 statement was required.) This is interpreted by the run-time system as an exit code indicating successful execution.\nData types.\nThe type system in C is static and weakly typed, which makes it similar to the type system of ALGOL descendants such as Pascal. There are built-in types for integers of various sizes, both signed and unsigned, floating-point numbers, and enumerated types (codice_8). Integer type codice_13 is often used for single-byte characters. C99 added a boolean datatype. There are also derived types including arrays, pointers, records (codice_6), and unions (codice_33).\nC is often used in low-level systems programming where escapes from the type system may be necessary. The compiler attempts to ensure type correctness of most expressions, but the programmer can override the checks in various ways, either by using a \"type cast\" to explicitly convert a value from one type to another, or by using pointers or unions to reinterpret the underlying bits of a data object in some other way.\nSome find C's declaration syntax unintuitive, particularly for function pointers. (Ritchie's idea was to declare identifiers in contexts resembling their use: \"declaration reflects use\".)\nC's \"usual arithmetic conversions\" allow for efficient code to be generated, but can sometimes produce unexpected results. For example, a comparison of signed and unsigned integers of equal width requires a conversion of the signed value to unsigned. This can generate unexpected results if the signed value is negative.\nPointers.\nC supports the use of pointers, a type of reference that records the address or location of an object or function in memory. Pointers can be \"dereferenced\" to access data stored at the address pointed to, or to invoke a pointed-to function. Pointers can be manipulated using assignment or pointer arithmetic. The run-time representation of a pointer value is typically a raw memory address (perhaps augmented by an offset-within-word field), but since a pointer's type includes the type of the thing pointed to, expressions including pointers can be type-checked at compile time. Pointer arithmetic is automatically scaled by the size of the pointed-to data type.\nPointers are used for many purposes in C. Text strings are commonly manipulated using pointers into arrays of characters. Dynamic memory allocation is performed using pointers; the result of a codice_207 is usually cast to the data type of the data to be stored. Many data types, such as trees, are commonly implemented as dynamically allocated codice_6 objects linked together using pointers. Pointers to other pointers are often used in multi-dimensional arrays and arrays of codice_6 objects. Pointers to functions (\"function pointers\") are useful for passing functions as arguments to higher-order functions (such as qsort or bsearch), in dispatch tables, or as callbacks to event handlers .\nA \"null pointer value\" explicitly points to no valid location. Dereferencing a null pointer value is undefined, often resulting in a segmentation fault. Null pointer values are useful for indicating special cases such as no \"next\" pointer in the final node of a linked list, or as an error indication from functions returning pointers. In appropriate contexts in source code, such as for assigning to a pointer variable, a \"null pointer constant\" can be written as codice_81, with or without explicit casting to a pointer type, or as the codice_211 macro defined by several standard headers. In conditional contexts, null pointer values evaluate to false, while all other pointer values evaluate to true.\nVoid pointers (codice_212) point to objects of unspecified type, and can therefore be used as \"generic\" data pointers. Since the size and type of the pointed-to object is not known, void pointers cannot be dereferenced, nor is pointer arithmetic on them allowed, although they can easily be (and in many contexts implicitly are) converted to and from any other object pointer type.\nCareless use of pointers is potentially dangerous. Because they are typically unchecked, a pointer variable can be made to point to any arbitrary location, which can cause undesirable effects. Although properly used pointers point to safe places, they can be made to point to unsafe places by using invalid pointer arithmetic; the objects they point to may continue to be used after deallocation (dangling pointers); they may be used without having been initialized (wild pointers); or they may be directly assigned an unsafe value using a cast, union, or through another corrupt pointer. In general, C is permissive in allowing manipulation of and conversion between pointer types, although compilers typically provide options for various levels of checking. Some other programming languages address these problems by using more restrictive reference types.\nArrays.\nArray types in C are traditionally of a fixed, static size specified at compile time. The more recent C99 standard also allows a form of variable-length arrays. However, it is also possible to allocate a block of memory (of arbitrary size) at run-time, using the standard library's codice_207 function, and treat it as an array.\nSince arrays are always accessed (in effect) via pointers, array accesses are typically \"not\" checked against the underlying array size, although some compilers may provide bounds checking as an option. Array bounds violations are therefore possible and can lead to various repercussions, including illegal memory accesses, corruption of data, buffer overruns, and run-time exceptions.\nC does not have a special provision for declaring multi-dimensional arrays, but rather relies on recursion within the type system to declare arrays of arrays, which effectively accomplishes the same thing. The index values of the resulting \"multi-dimensional array\" can be thought of as increasing in row-major order. Multi-dimensional arrays are commonly used in numerical algorithms (mainly from applied linear algebra) to store matrices. The structure of the C array is well suited to this particular task. However, in early versions of C the bounds of the array must be known fixed values or else explicitly passed to any subroutine that requires them, and dynamically sized arrays of arrays cannot be accessed using double indexing. (A workaround for this was to allocate the array with an additional \"row vector\" of pointers to the columns.) C99 introduced \"variable-length arrays\" which address this issue.\nThe following example using modern C (C99 or later) shows allocation of a two-dimensional array on the heap and the use of multi-dimensional array indexing for accesses (which can use bounds-checking on many C compilers):\nint func(int N, int M)\n float (*p)[N][M] = malloc(sizeof *p);\n if (!p)\n return -1;\n for (int i = 0; i < N; i++)\n for (int j = 0; j < M; j++)\n (*p)[i][j] = i + j;\n print_array(N, M, p);\n free(p);\n return 1;\nAnd here is a similar implementation using C99's \"Auto VLA\" feature:\nint func(int N, int M)\n // Caution: checks should be made to ensure N*M*sizeof(float) does NOT exceed limitations for auto VLAs and is within available size of stack.\n float p[N][M]; // auto VLA is held on the stack, and sized when the function is invoked\n for (int i = 0; i < N; i++)\n for (int j = 0; j < M; j++)\n p[i][j] = i + j;\n // no need to free(p) since it will disappear when the function exits, along with the rest of the stack frame\n return 1;\nArray\u2013pointer interchangeability.\nThe subscript notation codice_214 (where codice_215 designates a pointer) is syntactic sugar for codice_216. Taking advantage of the compiler's knowledge of the pointer type, the address that codice_217 points to is not the base address (pointed to by codice_215) incremented by codice_25 bytes, but rather is defined to be the base address incremented by codice_25 multiplied by the size of an element that codice_215 points to. Thus, codice_214 designates the codice_223th element of the array.\nFurthermore, in most expression contexts (a notable exception is as operand of codice_109), an expression of array type is automatically converted to a pointer to the array's first element. This implies that an array is never copied as a whole when named as an argument to a function, but rather only the address of its first element is passed. Therefore, although function calls in C use pass-by-value semantics, arrays are in effect passed by reference.\nThe total size of an array codice_215 can be determined by applying codice_109 to an expression of array type. The size of an element can be determined by applying the operator codice_109 to any dereferenced element of an array codice_79, as in codice_229. Thus, the number of elements in a declared array codice_79 can be determined as codice_231. Note, that if only a pointer to the first element is available as it is often the case in C code because of the automatic conversion described above, the information about the full type of the array and its length are lost.\nMemory management.\nOne of the most important functions of a programming language is to provide facilities for managing memory and the objects that are stored in memory. C provides three principal ways to allocate memory for objects:\nThese three approaches are appropriate in different situations and have various trade-offs. For example, static memory allocation has little allocation overhead, automatic allocation may involve slightly more overhead, and dynamic memory allocation can potentially have a great deal of overhead for both allocation and deallocation. The persistent nature of static objects is useful for maintaining state information across function calls, automatic allocation is easy to use but stack space is typically much more limited and transient than either static memory or heap space, and dynamic memory allocation allows convenient allocation of objects whose size is known only at run-time. Most C programs make extensive use of all three.\nWhere possible, automatic or static allocation is usually simplest because the storage is managed by the compiler, freeing the programmer of the potentially error-prone chore of manually allocating and releasing storage. However, many data structures can change in size at runtime, and since static allocations (and automatic allocations before C99) must have a fixed size at compile-time, there are many situations in which dynamic allocation is necessary. Prior to the C99 standard, variable-sized arrays were a common example of this. (See the article on codice_207 for an example of dynamically allocated arrays.) Unlike automatic allocation, which can fail at run time with uncontrolled consequences, the dynamic allocation functions return an indication (in the form of a null pointer value) when the required storage cannot be allocated. (Static allocation that is too large is usually detected by the linker or loader, before the program can even begin execution.)\nUnless otherwise specified, static objects contain zero or null pointer values upon program startup. Automatically and dynamically allocated objects are initialized only if an initial value is explicitly specified; otherwise they initially have indeterminate values (typically, whatever bit pattern happens to be present in the storage, which might not even represent a valid value for that type). If the program attempts to access an uninitialized value, the results are undefined. Many modern compilers try to detect and warn about this problem, but both false positives and false negatives can occur.\nHeap memory allocation has to be synchronized with its actual usage in any program to be reused as much as possible. For example, if the only pointer to a heap memory allocation goes out of scope or has its value overwritten before it is deallocated explicitly, then that memory cannot be recovered for later reuse and is essentially lost to the program, a phenomenon known as a \"memory leak.\" Conversely, it is possible for memory to be freed, but is referenced subsequently, leading to unpredictable results. Typically, the failure symptoms appear in a portion of the program unrelated to the code that causes the error, making it difficult to diagnose the failure. Such issues are ameliorated in languages with automatic garbage collection.\nLibraries.\nThe C programming language uses libraries as its primary method of extension. In C, a library is a set of functions contained within a single \"archive\" file. Each library typically has a header file, which contains the prototypes of the functions contained within the library that may be used by a program, and declarations of special data types and macro symbols used with these functions. In order for a program to use a library, it must include the library's header file, and the library must be linked with the program, which in many cases requires compiler flags (e.g., codice_236, shorthand for \"link the math library\").\nThe most common C library is the C standard library, which is specified by the ISO and ANSI C standards and comes with every C implementation (implementations which target limited environments such as embedded systems may provide only a subset of the standard library). This library supports stream input and output, memory allocation, mathematics, character strings, and time values. Several separate standard headers (for example, codice_176) specify the interfaces for these and other standard library facilities.\nAnother common set of C library functions are those used by applications specifically targeted for Unix and Unix-like systems, especially functions which provide an interface to the kernel. These functions are detailed in various standards such as POSIX and the Single UNIX Specification.\nSince many programs have been written in C, there are a wide variety of other libraries available. Libraries are often written in C because C compilers generate efficient object code; programmers then create interfaces to the library so that the routines can be used from higher-level languages like Java, Perl, and Python.\nFile handling and streams.\nFile input and output (I/O) is not part of the C language itself but instead is handled by libraries (such as the C standard library) and their associated header files (e.g. codice_176). File handling is generally implemented through high-level I/O which works through streams. A stream is from this perspective a data flow that is independent of devices, while a file is a concrete device. The high-level I/O is done through the association of a stream to a file. In the C standard library, a buffer (a memory area or queue) is temporarily used to store data before it is sent to the final destination. This reduces the time spent waiting for slower devices, for example a hard drive or solid state drive. Low-level I/O functions are not part of the standard C library but are generally part of \"bare metal\" programming (programming that's independent of any operating system such as most embedded programming). With few exceptions, implementations include low-level I/O.\nLanguage tools.\nA number of tools have been developed to help C programmers find and fix statements with undefined behavior or possibly erroneous expressions, with greater rigor than that provided by the compiler. The tool lint was the first such, leading to many others.\nAutomated source code checking and auditing are beneficial in any language, and for C many such tools exist, such as Lint. A common practice is to use Lint to detect questionable code when a program is first written. Once a program passes Lint, it is then compiled using the C compiler. Also, many compilers can optionally warn about syntactically valid constructs that are likely to actually be errors. MISRA C is a proprietary set of guidelines to avoid such questionable code, developed for embedded systems.\nThere are also compilers, libraries, and operating system level mechanisms for performing actions that are not a standard part of C, such as bounds checking for arrays, detection of buffer overflow, serialization, dynamic memory tracking, and automatic garbage collection.\nTools such as Purify or Valgrind and linking with libraries containing special versions of the memory allocation functions can help uncover runtime errors in memory usage.\nUses.\nRationale for use in systems programming.\nC is widely used for systems programming in implementing operating systems and embedded system applications. This is for several reasons:\nOnce used for web development.\nHistorically, C was sometimes used for web development using the Common Gateway Interface (CGI) as a \"gateway\" for information between the web application, the server, and the browser. C may have been chosen over interpreted languages because of its speed, stability, and near-universal availability. It is no longer common practice for web development to be done in C, and many other web development tools exist.\nSome other languages are themselves written in C.\nA consequence of C's wide availability and efficiency is that compilers, libraries and interpreters of other programming languages are often implemented in C. For example, the reference implementations of Python, Perl, Ruby, and PHP are written in C.\nUsed for computationally-intensive libraries.\nC enables programmers to create efficient implementations of algorithms and data structures, because the layer of abstraction from hardware is thin, and its overhead is low, an important criterion for computationally intensive programs. For example, the GNU Multiple Precision Arithmetic Library, the GNU Scientific Library, Mathematica, and MATLAB are completely or partially written in C. Many languages support calling library functions in C, for example, the Python-based framework NumPy uses C for the high-performance and hardware-interacting aspects.\nC as an intermediate language.\nC is sometimes used as an intermediate language by implementations of other languages. This approach may be used for portability or convenience; by using C as an intermediate language, additional machine-specific code generators are not necessary. C has some features, such as line-number preprocessor directives and optional superfluous commas at the end of initializer lists, that support compilation of generated code. However, some of C's shortcomings have prompted the development of other C-based languages specifically designed for use as intermediate languages, such as C--. Also, contemporary major compilers GCC and LLVM both feature an intermediate representation that is not C, and those compilers support front ends for many languages including C.\nEnd-user applications.\nC has also been widely used to implement end-user applications. However, such applications can also be written in newer, higher-level languages.\nLimitations.\nWhile C has been popular, influential and hugely successful, it has drawbacks, including:\nFor some purposes, restricted styles of C have been adopted, e.g. MISRA C or CERT C, in an attempt to reduce the opportunity for bugs. Databases such as CWE attempt to count the ways C etc. has vulnerabilities, along with recommendations for mitigation.\nThere are tools that can mitigate against some of the drawbacks. Contemporary C compilers include checks which may generate warnings to help identify many potential bugs.\nSome of these drawbacks have prompted the construction of other languages.\nRelated languages.\nC has both directly and indirectly influenced many later languages such as C++ and Java. The most pervasive influence has been syntactical; all of the languages mentioned combine the statement and (more or less recognizably) expression syntax of C with type systems, data models or large-scale program structures that differ from those of C, sometimes radically.\nSeveral C or near-C interpreters exist, including Ch and CINT, which can also be used for scripting.\nWhen object-oriented programming languages became popular, C++ and Objective-C were two different extensions of C that provided object-oriented capabilities. Both languages were originally implemented as source-to-source compilers; source code was translated into C, and then compiled with a C compiler.\nThe C++ programming language (originally named \"C with Classes\") was devised by Bjarne Stroustrup as an approach to providing object-oriented functionality with a C-like syntax. C++ adds greater typing strength, scoping, and other tools useful in object-oriented programming, and permits generic programming via templates. Nearly a superset of C, C++ now supports most of C, with a few exceptions.\nObjective-C was originally a very \"thin\" layer on top of C, and remains a strict superset of C that permits object-oriented programming using a hybrid dynamic/static typing paradigm. Objective-C derives its syntax from both C and Smalltalk: syntax that involves preprocessing, expressions, function declarations, and function calls is inherited from C, while the syntax for object-oriented features was originally taken from Smalltalk.\nIn addition to C++ and Objective-C, Ch, Cilk, and Unified Parallel C are nearly supersets of C.", "categories": ["Category:All Wikipedia articles in need of updating", "Category:All articles needing additional references", "Category:All articles with unsourced statements", "Category:All articles with vague or ambiguous time", "Category:American inventions", "Category:Articles needing additional references from July 2014", "Category:Articles needing additional references from October 2012", "Category:Articles with BNF identifiers", "Category:Articles with BNFdata identifiers", "Category:Articles with FAST identifiers"]}
{"docid": 7392, "title": "Class (computer programming)", "text": "In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. Although, to the point of conflation, one could argue that is a feature inherent in a language because of its polymorphic nature and why these languages are so powerful, dynamic and adaptable for use compared to languages without polymorphism present. Thus they can model dynamic systems (i.e. the real world, machine learning, AI) more easily.\nWhen an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class.\nIn certain languages, classes are, as a matter of fact, only a compile-time feature (new classes cannot be declared at run-time), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type or similar). In these languages, a class that creates classes within itself is called a metaclass.\nClass vs. type.\nIn its most casual usage, people often refer to the \"class\" of an object, but narrowly speaking objects have \"type\": the interface, namely the types of member variables, the signatures of member functions (methods), and properties these satisfy. At the same time, a class has an implementation (specifically the implementation of the methods), and can create objects of a given type, with a given implementation. In the terms of type theory, a class is an implementationa \"concrete\" data structure and collection of subroutineswhile a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system); for example, the type might be implemented with two classes (fast for small stacks, but scales poorly) and (scales well but high overhead for small stacks). Similarly, a given class may have several different constructors.\nClass types generally represent nouns, such as a person, place or thing, or something nominalized, and a class represents an implementation of these. For example, a type might represent the properties and functionality of bananas in general, while the and classes would represent ways of producing bananas (say, banana suppliers or data structures and functions to represent and draw bananas in a video game). The class could then produce particular bananas: instances of the class would be objects of type . Often only a single implementation of a type is given, in which case the class name is often identical with the type name.\nDesign and implementation.\nClasses are composed from structural and behavioral constituents. Programming languages that include classes as a programming construct offer support, for various class-related features, and the syntax required to use these features varies greatly from one programming language to another.\nStructure.\nA class contains data field descriptions (or \"properties\", \"fields\", \"data members\", or \"attributes\"). These are usually field types and names that will be associated with state variables at program run time; these state variables either belong to the class or specific instances of the class. In most languages, the structure defined by the class determines the layout of the memory used by its instances. Other implementations are possible: for example, objects in Python use associative key-value containers.\nSome programming languages such as Eiffel support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.\nBehavior.\nThe behavior of class or its instances is defined using methods. Methods are subroutines with the ability to operate on objects or classes. These operations may alter the state of an object or simply provide ways of accessing it. Many kinds of methods exist, but support for them varies across languages. Some types of methods are created and called by programmer code, while other special methods\u2014such as constructors, destructors, and conversion operators\u2014are created and called by compiler-generated code. A language may also allow the programmer to define and call these special methods.\nThe concept of class interface.\nEvery class \"implements\" (or \"realizes\") an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented. There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing implementation.\nLanguages that support class inheritance also allow classes to inherit interfaces from the classes that they are derived from.\nFor example, if \"class A\" inherits from \"class B\" and if \"class B\" implements the interface \"interface B\" then \"class A\" also inherits the functionality(constants and methods declaration) provided by \"interface B\".\nIn languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.\nObject-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object. \nExample.\nThe buttons on the front of your television set are the interface between you and the electrical wiring on the other side of its plastic casing. You press the \"power\" button to toggle the television on and off. In this example, your particular television is the instance, each method is represented by a button, and all the buttons together compose the interface (other television sets that are the same model as yours would have the same interface). In its most common form, an interface is a specification of a group of related methods without any associated implementation of the methods.\nA television set also has a myriad of \"attributes\", such as size and whether it supports colour, which together comprise its structure. A class represents the full description of a television, including its attributes (structure) and buttons (interface).\nGetting the total number of televisions manufactured could be a \"static method\" of the television class. This method is clearly associated with the class, yet is outside the domain of each individual instance of the class. A static method that finds a particular instance out of the set of all television objects is another example.\nMember accessibility.\nThe following is a common set of access specifiers:\nAlthough many object-oriented languages support the above access specifiers, their semantics may differ.\nObject-oriented design uses the access specifiers in conjunction with careful design of public method implementations to enforce class invariants\u2014constraints on the state of the objects. A common usage of access specifiers is to separate the internal data of a class from its interface: the internal structure is made private, while public accessor methods can be used to inspect or alter such private data.\nAccess specifiers do not necessarily control \"visibility\", in that even private members may be visible to client external code. In some languages, an inaccessible but visible member may be referred to at run-time (for example, by a pointer returned from a member function), but an attempt to use it by referring to the name of the member from client code will be prevented by the type checker.\nThe various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile-time or run-time. For example, the Java language does not allow client code that accesses the private data of a class to compile.\n In the C++ language, private methods are visible, but not accessible in the interface; however, they may be made invisible by explicitly declaring fully abstract classes that represent the interfaces of the class.\nSome languages feature other accessibility schemes:\nInter-class relationships.\nIn addition to the design of standalone classes, programming languages may support more advanced class design based upon relationships between classes. The inter-class relationship design capabilities commonly provided are \"compositional\" and \"hierarchical\".\nCompositional.\nClasses can be composed of other classes, thereby establishing a compositional relationship between the enclosing class and its embedded classes. Compositional relationship between classes is also commonly known as a \"has-a\" relationship. For example, a class \"Car\" could be composed of and contain a class \"Engine\". Therefore, a Car \"has an\" Engine. One aspect of composition is containment, which is the enclosure of component instances by the instance that has them. If an enclosing object contains component instances by value, the components and their enclosing object have a similar lifetime. If the components are contained by reference, they may not have a similar lifetime. For example, in Objective-C 2.0:\n@interface Car : NSObject\n@property NSString *name;\n@property Engine *engine\n@property NSArray *tires;\n@end\nThis class \"has\" an instance of (a string object), , and (an array object).\nHierarchical.\nClasses can be \"derived\" from one or more existing classes, thereby establishing a hierarchical relationship between the derived-from classes (\"base classes\", \"parent classes\" or ') and the derived class (\"child class\" or \"subclass\") . The relationship of the derived class to the derived-from classes is commonly known as an is-a relationship. For example, a class 'Button' could be derived from a class 'Control'. Therefore, a Button is a\"' Control. Structural and behavioral members of the parent classes are \"inherited\" by the child class. Derived classes can define additional structural members (data fields) and behavioral members (methods) in addition to those that they \"inherit\" and are therefore \"specializations\" of their superclasses. Also, derived classes can override inherited methods if the language allows.\nNot all languages support multiple inheritance. For example, Java allows a class to implement multiple interfaces, but only inherit from one class. If multiple inheritance is allowed, the hierarchy is a directed acyclic graph (or DAG for short), otherwise it is a tree. The hierarchy has classes as nodes and inheritance relationships as links. Classes in the same level are more likely to be associated than classes in different levels. The levels of this hierarchy are called layers or levels of abstraction.\nExample (Simplified Objective-C 2.0 code, from iPhone SDK):\n@interface UIResponder : NSObject //...\n@interface UIView : UIResponder //...\n@interface UIScrollView : UIView //...\n@interface UITableView : UIScrollView //...\nIn this example, a UITableView is a UIScrollView is a UIView is a UIResponder is an NSObject.\nDefinitions of subclass.\nConceptually, a superclass is a superset of its subclasses. For example, a common class hierarchy would involve as a superclass of and , while would be a subclass of . These are all subset relations in set theory as well, i.e., all squares are rectangles but not all rectangles are squares.\nA common conceptual error is to mistake a \"part of\" relation with a subclass. For example, a car and truck are both kinds of vehicles and it would be appropriate to model them as subclasses of a vehicle class. However, it would be an error to model the component parts of the car as subclass relations. For example, a car is composed of an engine and body, but it would not be appropriate to model engine or body as a subclass of car.\nIn object-oriented modeling these kinds of relations are typically modeled as object properties. In this example, the class would have a property called . would be typed to hold a collection of objects, such as instances of , , , etc.\nObject modeling languages such as UML include capabilities to model various aspects of \"part of\" and other kinds of relations \u2013 data such as the cardinality of the objects, constraints on input and output values, etc. This information can be utilized by developer tools to generate additional code beside the basic data definitions for the objects, such as error checking on get and set methods.\nOne important question when modeling and implementing a system of object classes is whether a class can have one or more superclasses. In the real world with actual sets it would be rare to find sets that did not intersect with more than one other set. However, while some systems such as Flavors and CLOS provide a capability for more than one parent to do so at run time introduces complexity that many in the object-oriented community consider antithetical to the goals of using object classes in the first place. Understanding which class will be responsible for handling a message can get complex when dealing with more than one superclass. If used carelessly this feature can introduce some of the same system complexity and ambiguity classes were designed to avoid.\nMost modern object-oriented languages such as Smalltalk and Java require single inheritance at run time. For these languages, multiple inheritance may be useful for modeling but not for an implementation.\nHowever, semantic web application objects do have multiple superclasses. The volatility of the Internet requires this level of flexibility and the technology standards such as the Web Ontology Language (OWL) are designed to support it.\nA similar issue is whether or not the class hierarchy can be modified at run time. Languages such as Flavors, CLOS, and Smalltalk all support this feature as part of their meta-object protocols. Since classes are themselves first-class objects, it is possible to have them dynamically alter their structure by sending them the appropriate messages. Other languages that focus more on strong typing such as Java and C++ do not allow the class hierarchy to be modified at run time. Semantic web objects have the capability for run time changes to classes. The rational is similar to the justification for allowing multiple superclasses, that the Internet is so dynamic and flexible that dynamic changes to the hierarchy are required to manage this volatility.\nOrthogonality of the class concept and inheritance.\nAlthough class-based languages are commonly assumed to support inheritance, inheritance is not an intrinsic aspect of the concept of classes. Some languages, often referred to as \"object-based languages\", support classes yet do not support inheritance. Examples of object-based languages include earlier versions of Visual Basic.\nWithin object-oriented analysis.\nIn object-oriented analysis and in UML, an association between two classes represents a collaboration between the classes or their corresponding instances. Associations have direction; for example, a bi-directional association between two classes indicates that both of the classes are aware of their relationship. Associations may be labeled according to their name or purpose.\nAn association role is given end of an association and describes the role of the corresponding class. For example, a \"subscriber\" role describes the way instances of the class \"Person\" participate in a \"subscribes-to\" association with the class \"Magazine\". Also, a \"Magazine\" has the \"subscribed magazine\" role in the same association. Association role multiplicity describes how many instances correspond to each instance of the other class of the association. Common multiplicities are \"0..1\", \"1..1\", \"1..*\" and \"0..*\", where the \"*\" specifies any number of instances.\nTaxonomy of classes.\nThere are many categories of classes, some of which overlap.\nAbstract and concrete.\nIn a language that supports inheritance, an abstract class, or abstract base class (ABC), is a class that cannot be instantiated because it is either labeled as abstract or it simply specifies abstract methods (or \"virtual methods\"). An abstract class may provide implementations of some methods, and may also specify virtual methods via signatures that are to be implemented by direct or indirect descendants of the abstract class. Before a class derived from an abstract class can be instantiated, all abstract methods of its parent classes must be implemented by some class in the derivation chain.\nMost object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword \"abstract\" is used. In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).\nA class consisting of only virtual methods is called a Pure Abstract Base Class (or \"Pure ABC\") in C++ and is also known as an \"interface\" by users of the language. Other languages, notably Java and C#, support a variant of abstract classes called an interface via a keyword in the language. In these languages, multiple inheritance is not allowed, but a class can implement multiple interfaces. Such a class can only contain abstract publicly accessible methods.\nA concrete class is a class that can be instantiated, as opposed to abstract classes, which cannot. \nLocal and inner.\nIn some languages, classes can be declared in scopes other than the global scope. There are various types of such classes.\nAn inner class is a class defined within another class. The relationship between an inner class and its containing class can also be treated as another type of class association. An inner class is typically neither associated with instances of the enclosing class nor instantiated along with its enclosing class. Depending on language, it may or may not be possible to refer to the class from outside the enclosing class. A related concept is \"inner types\", also known as \"inner data type\" or \"nested type\", which is a generalization of the concept of inner classes. C++ is an example of a language that supports both inner classes and inner types (via \"typedef\" declarations).\nAnother type is a local class, which is a class defined within a procedure or function. This limits references to the class name to within the scope where the class is declared. Depending on the semantic rules of the language, there may be additional restrictions on local classes compared to non-local ones. One common restriction is to disallow local class methods to access local variables of the enclosing function. For example, in C++, a local class may refer to static variables declared within its enclosing function, but may not access the function's automatic variables.\nMetaclasses.\nMetaclasses are classes whose instances are classes. A metaclass describes a common structure of a collection of classes and can implement a design pattern or describe particular kinds of classes. Metaclasses are often used to describe frameworks.\nIn some languages, such as Python, Ruby or Smalltalk, a class is also an object; thus each class is an instance of a unique metaclass that is built into the language.\nThe Common Lisp Object System (CLOS) provides metaobject protocols (MOPs) to implement those classes and metaclasses.\nNon-subclassable.\nNon-subclassable classes allow programmers to design classes and hierarchies of classes where at some level in the hierarchy, further derivation is prohibited (a stand-alone class may be also designated as non-subclassable, preventing the formation of any hierarchy). Contrast this to \"abstract\" classes, which imply, encourage, and require derivation in order to be used at all. A non-subclassable class is implicitly \"concrete\".\nA non-subclassable class is created by declaring the class as in C# or as in Java or PHP. For example, Java's class is designated as \"final\".\nNon-subclassable classes may allow a compiler (in compiled languages) to perform optimizations that are not available for subclassable classes. \nOpen class.\nAn open class is one that can be changed. Typically, an executable program cannot be changed by customers. Developers can often change some classes, but typically cannot change standard or built-in ones. In Ruby, all classes are open. In Python, classes can be created at runtime, and all can be modified afterwards. Objective-C categories permit the programmer to add methods to an existing class without the need to recompile that class or even have access to its source code.\nMixins.\nSome languages have special support for mixins, though in any language with multiple inheritance a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes; for example, a class might provide a method called when included in classes and that do not share a common parent.\nPartial.\nIn languages supporting the feature, a partial class is a class whose definition may be split into multiple pieces, within a single source-code file or across multiple files. The pieces are merged at compile-time, making compiler output the same as for a non-partial class.\nThe primary motivation for introduction of partial classes is to facilitate the implementation of code generators, such as visual designers. It is otherwise a challenge or compromise to develop code generators that can manage the generated code when it is interleaved within developer-written code. Using partial classes, a code generator can process a separate file or coarse-grained partial class within a file, and is thus alleviated from intricately interjecting generated code via extensive parsing, increasing compiler efficiency and eliminating the potential risk of corrupting developer code. In a simple implementation of partial classes, the compiler can perform a phase of precompilation where it \"unifies\" all the parts of a partial class. Then, compilation can proceed as usual.\nOther benefits and effects of the partial class feature include:\nPartial classes have existed in Smalltalk under the name of \"Class Extensions\" for considerable time. With the arrival of the .NET framework 2, Microsoft introduced partial classes, supported in both C# 2.0 and Visual Basic 2005. WinRT also supports partial classes.\nExample in VB.NET.\nThis simple example, written in Visual Basic .NET, shows how parts of the same class are defined in two different files.\nPartial Class MyClass\n Private _name As String\nEnd Class\nPartial Class MyClass\n Public Readonly Property Name() As String\n Get\n Return _name\n End Get\n End Property\nEnd Class\nWhen compiled, the result is the same as if the two files were written as one, like this:\nClass MyClass\n Private _name As String\n Public Readonly Property Name() As String\n Get\n Return _name\n End Get\n End Property\nEnd Class\nExample in Objective-C.\nIn Objective-C, partial classes, also known as categories, may even spread over multiple libraries and executables, like the following example. But a key difference is that Objective-C's categories can overwrite definitions in another interface declaration, and that categories are not equal to original class definition (the first requires the last). Instead, .NET partial class can not have conflicting definitions, and all partial definitions are equal to the others.\nIn Foundation, header file NSData.h:\n@interface NSData : NSObject\n- (id)initWithContentsOfURL:(NSURL *)URL;\n@end\nIn user-supplied library, a separate binary from Foundation framework, header file NSData+base64.h:\n@interface NSData (base64)\n- (NSString *)base64String;\n- (id)initWithBase64String:(NSString *)base64String;\n@end\nAnd in an app, yet another separate binary file, source code file main.m:\nint main(int argc, char *argv[])\n if (argc < 2)\n return EXIT_FAILURE;\n NSString *sourceURLString = [NSString stringWithCString:argv[1]];\n NSData *data = ;\n NSLog(@\"%@\", [data base64String]);\n return EXIT_SUCCESS;\nThe dispatcher will find both methods called over the NSData instance and invoke both of them correctly.\nUninstantiable.\nUninstantiable classes allow programmers to group together per-class fields and methods that are accessible at runtime without an instance of the class. Indeed, instantiation is prohibited for this kind of class.\nFor example, in C#, a class marked \"static\" can not be instantiated, can only have static members (fields, methods, other), may not have \"instance constructors\", and is \"sealed\".\nUnnamed.\nAn unnamed class or anonymous class is a class that is not bound to a name or identifier upon definition. This is analogous to named versus unnamed functions.\nBenefits.\nThe benefits of organizing software into object classes fall into three categories:\nObject classes facilitate rapid development because they lessen the semantic gap between the code and the users. System analysts can talk to both developers and users using essentially the same vocabulary, talking about accounts, customers, bills, etc. Object classes often facilitate rapid development because most object-oriented environments come with powerful debugging and testing tools. Instances of classes can be inspected at run time to verify that the system is performing as expected. Also, rather than get dumps of core memory, most object-oriented environments have interpreted debugging capabilities so that the developer can analyze exactly where in the program the error occurred and can see which methods were called to which arguments and with what arguments.\nObject classes facilitate ease of maintenance via encapsulation. When developers need to change the behavior of an object they can localize the change to just that object and its component parts. This reduces the potential for unwanted side effects from maintenance enhancements.\nSoftware re-use is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailor some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.\nRun-time representation.\nAs a data type, a class is usually considered as a compile-time construct. A language or library may also support prototype or factory metaobjects that represent run-time information about classes, or even represent metadata that provides access to reflection facilities and ability to manipulate data structure formats at run-time. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at run-time. Some dynamic languages do not make strict distinctions between run-time and compile-time constructs, and therefore may not distinguish between metaobjects and classes.\nFor example, if Human is a metaobject representing the class Person, then instances of class Person can be created by using the facilities of the Human metaobject.", "categories": ["Category:All articles needing additional references", "Category:All articles with unsourced statements", "Category:Articles needing additional references from April 2012", "Category:Articles needing additional references from May 2012", "Category:Articles with example Java code", "Category:Articles with short description", "Category:Articles with unsourced statements from April 2012", "Category:Class (computer programming)", "Category:Programming constructs", "Category:Programming language topics"]}
{"docid": 27471338, "title": "Object-oriented programming", "text": "Object-Oriented Programming (OOP) is a programming paradigm based on the concept of \"objects\", which can contain data and code. The data is in the form of fields (often known as attributes or \"properties\"), and the code is in the form of procedures (often known as \"methods\"). \nA common feature of objects is that procedures (or methods) are attached to them and can access and modify the object's data fields. In this brand of OOP, there is usually a special name such as or used to refer to the current object. In OOP, computer programs are designed by making them out of objects that interact with one another. OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.\nMany of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. \nSignificant object-oriented languages include: Ada, ActionScript, C++, Common Lisp, C#, Dart, Eiffel, Fortran 2003, Haxe, Java, JavaScript, Kotlin, logo, MATLAB, Objective-C, Object Pascal, Perl, PHP, Python, R, Raku, Ruby, Scala, SIMSCRIPT, Simula, Smalltalk, Swift, Vala and Visual Basic.NET.\nHistory.\nTerminology invoking \"objects\" and \"oriented\" in the modern sense of object-oriented programming made its first appearance at MIT in the late 1950s and early 1960s. In the environment of the artificial intelligence group, as early as 1960, \"object\" could refer to identified items (LISP atoms) with properties (attributes);\nAlan Kay later cited a detailed understanding of LISP internals as a strong influence on his thinking in 1966.\nAnother early MIT example was Sketchpad created by Ivan Sutherland in 1960\u20131961; in the glossary of the 1963 technical report based on his dissertation about Sketchpad, Sutherland defined notions of \"object\" and \"instance\" (with the class concept covered by \"master\" or \"definition\"), albeit specialized to graphical interaction.\nAlso, an MIT ALGOL version, AED-0, established a direct link between data structures (\"plexes\", in that dialect) and procedures, prefiguring what were later termed \"messages\", \"methods\", and \"member functions\".\nSimula introduced important concepts that are today an essential part of object-oriented programming, such as class and object, inheritance, and dynamic binding. \nThe object-oriented Simula programming language was used mainly by researchers involved with physical modelling, such as models to study and improve the movement of ships and their content through cargo ports.\nIn the 1970s, the first version of the Smalltalk programming language was developed at Xerox PARC by Alan Kay, Dan Ingalls and Adele Goldberg. Smalltalk-72 included a programming environment and was dynamically typed, and at first was interpreted, not compiled. Smalltalk became noted for its application of object orientation at the language-level and its graphical development environment. Smalltalk went through various versions and interest in the language grew. While Smalltalk was influenced by the ideas introduced in Simula 67 it was designed to be a fully dynamic system in which classes could be created and modified dynamically.\nIn the 1970s, Smalltalk influenced the Lisp community to incorporate object-based techniques that were introduced to developers via the Lisp machine. Experimentation with various extensions to Lisp (such as LOOPS and Flavors introducing multiple inheritance and mixins) eventually led to the Common Lisp Object System, which integrates functional programming and object-oriented programming and allows extension via a Meta-object protocol. In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.\nIn 1981, Goldberg edited the August issue of Byte Magazine, introducing Smalltalk and object-oriented programming to a wider audience. In 1986, the Association for Computing Machinery organised the first \"Conference on Object-Oriented Programming, Systems, Languages, and Applications\" (OOPSLA), which was unexpectedly attended by 1,000 people. In the mid-1980s Objective-C was developed by Brad Cox, who had used Smalltalk at ITT Inc., and Bjarne Stroustrup, who had used Simula for his PhD thesis, eventually went to create the object-oriented C++. In 1985, Bertrand Meyer also produced the first design of the Eiffel language. Focused on software quality, Eiffel is a purely object-oriented programming language and a notation supporting the entire software lifecycle. Meyer described the Eiffel software development method, based on a small number of key ideas from software engineering and computer science, in Object-Oriented Software Construction. Essential to the quality focus of Eiffel is Meyer's reliability mechanism, Design by Contract, which is an integral part of both the method and language.\nIn the early and mid-1990s object-oriented programming developed as the dominant programming paradigm when programming languages supporting the techniques became widely available. These included Visual FoxPro 3.0, C++, and Delphi. Its dominance was further enhanced by the rising popularity of graphical user interfaces, which rely heavily upon object-oriented programming techniques. An example of a closely related dynamic GUI library and OOP language can be found in the Cocoa frameworks on Mac OS X, written in Objective-C, an object-oriented, dynamic messaging extension to C based on Smalltalk. OOP toolkits also enhanced the popularity of event-driven programming (although this concept is not limited to OOP).\nAt ETH Z\u00fcrich, Niklaus Wirth and his colleagues had also been investigating such topics as data abstraction and modular programming (although this had been in common use in the 1960s or earlier). Modula-2 (1978) included both, and their succeeding design, Oberon, included a distinctive approach to object orientation, classes, and such.\nObject-oriented features have been added to many previously existing languages, including Ada, BASIC, Fortran, Pascal, and COBOL. Adding these features to languages that were not initially designed for them often led to problems with compatibility and maintainability of code.\nMore recently, a number of languages have emerged that are primarily object-oriented, but that are also compatible with procedural methodology. Two such languages are Python and Ruby. Probably the most commercially important recent object-oriented languages are Java, developed by Sun Microsystems, as well as C# and Visual Basic.NET (VB.NET), both designed for Microsoft's .NET platform. Each of these two frameworks shows, in its own way, the benefit of using OOP by creating an abstraction from implementation. VB.NET and C# support cross-language inheritance, allowing classes defined in one language to subclass classes defined in the other language.\nFeatures.\nObject-oriented programming uses objects, but not all of the associated techniques and structures are supported directly in languages that claim to support OOP. It performs operations on operands. The features listed below are common among languages considered to be strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.\nShared with non-OOP languages.\nModular programming support provides the ability to group procedures into files and modules for organizational purposes. Modules are namespaced so identifiers in one module will not conflict with a procedure or variable sharing the same name in another file or module.\nObjects and classes.\nLanguages that support object-oriented programming (OOP) typically use inheritance for code reuse and extensibility in the form of either classes or prototypes. Those that use classes support two main concepts:\nObjects sometimes correspond to things found in the real world. For example, a graphics program may have objects such as \"circle\", \"square\", \"menu\". An online shopping system might have objects such as \"shopping cart\", \"customer\", and \"product\". Sometimes objects represent more abstract entities, like an object that represents an open file, or an object that provides the service of translating measurements from U.S. customary to metric.\nEach object is said to be an instance of a particular class (for example, an object with its name field set to \"Mary\" might be an instance of class Employee). Procedures in object-oriented programming are known as methods; variables are also known as fields, members, attributes, or properties. This leads to the following terms:\nObjects are accessed somewhat like variables with complex internal structure, and in many languages are effectively pointers, serving as actual references to a single instance of said object in memory within a heap or stack. They provide a layer of abstraction which can be used to separate internal from external code. External code can use an object by calling a specific instance method with a certain set of input parameters, read an instance variable, or write to an instance variable. Objects are created by calling a special type of method in the class known as a constructor. A program may create many instances of the same class as it runs, which operate independently. This is an easy way for the same procedures to be used on different sets of data.\nObject-oriented programming that uses classes is sometimes called class-based programming, while prototype-based programming does not typically use classes. As a result, significantly different yet analogous terminology is used to define the concepts of \"object\" and \"instance\".\nIn some languages classes and objects can be composed using other concepts like traits and mixins.\nClass-based vs prototype-based.\nIn class-based languages the \"classes\" are defined beforehand and the \"objects\" are instantiated based on the classes. If two objects \"apple\" and \"orange\" are instantiated from the class \"Fruit\", they are inherently fruits and it is guaranteed that you may handle them in the same way; e.g. a programmer can expect the existence of the same attributes such as \"color\" or \"sugar_content\" or \"is_ripe\".\nIn prototype-based languages the \"objects\" are the primary entities. No \"classes\" even exist. The \"prototype\" of an object is just another object to which the object is linked. Every object has one \"prototype\" link (and only one). New objects can be created based on already existing objects chosen as their prototype. You may call two different objects \"apple\" and \"orange\" a fruit, if the object \"fruit\" exists, and both \"apple\" and \"orange\" have \"fruit\" as their prototype. The idea of the \"fruit\" class doesn't exist explicitly, but as the equivalence class of the objects sharing the same prototype. The attributes and methods of the \"prototype\" are delegated to all the objects of the equivalence class defined by this prototype. The attributes and methods \"owned\" individually by the object may not be shared by other objects of the same equivalence class; e.g. the attribute \"sugar_content\" may be unexpectedly not present in \"apple\". Only single inheritance can be implemented through the prototype.\nDynamic dispatch/message passing.\nIt is the responsibility of the object, not any external code, to select the procedural code to execute in response to a method call, typically by looking up the method at run time in a table associated with the object. This feature is known as dynamic dispatch. If the call variability relies on more than the single type of the object on which it is called (i.e. at least one other parameter object is involved in the method choice), one speaks of multiple dispatch.\nA method call is also known as \"message passing\". It is conceptualized as a message (the name of the method and its input parameters) being passed to the object for dispatch.\nData abstraction.\nData abstraction is a design pattern in which data are visible only to semantically related functions, so as to prevent misuse. The success of data abstraction leads to frequent incorporation of data hiding as a design principle in object oriented and pure functional programming.\nIf a class does not allow calling code to access internal object data and permits access through methods only, this is a form of information hiding known as abstraction. Some languages (Java, for example) let classes enforce access restrictions explicitly, for example denoting internal data with the codice_1 keyword and designating methods intended for use by code outside the class with the codice_2 keyword. Methods may also be designed public, private, or intermediate levels such as codice_3 (which allows access from the same class and its subclasses, but not objects of a different class). In other languages (like Python) this is enforced only by convention (for example, codice_1 methods may have names that start with an underscore). In C#, Swift & Kotlin languages, codice_5 keyword permits access only to files present in same assembly, package or module as that of the class.\nEncapsulation.\nEncapsulation prevents external code from being concerned with the internal workings of an object. This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code (as long as \"public\" method calls work the same way). It also encourages programmers to put all the code that is concerned with a certain set of data in the same class, which organizes it for easy comprehension by other programmers. Encapsulation is a technique that encourages decoupling.\nComposition, inheritance, and delegation.\nObjects can contain other objects in their instance variables; this is known as object composition. For example, an object in the Employee class might contain (either directly or through a pointer) an object in the Address class, in addition to its own instance variables like \"first_name\" and \"position\". Object composition is used to represent \"has-a\" relationships: every employee has an address, so every Employee object has access to a place to store an Address object (either directly embedded within itself, or at a separate location addressed via a pointer).\nLanguages that support classes almost always support inheritance. This allows classes to be arranged in a hierarchy that represents \"is-a-type-of\" relationships. For example, class Employee might inherit from class Person. All the data and methods available to the parent class also appear in the child class with the same names. For example, class Person might define variables \"first_name\" and \"last_name\" with method \"make_full_name()\". These will also be available in class Employee, which might add the variables \"position\" and \"salary\". This technique allows easy re-use of the same procedures and data definitions, in addition to potentially mirroring real-world relationships in an intuitive way. Rather than utilizing database tables and programming subroutines, the developer utilizes objects the user may be more familiar with: objects from their application domain.\nSubclasses can override the methods defined by superclasses. Multiple inheritance is allowed in some languages, though this can make resolving overrides complicated. Some languages have special support for mixins, though in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes. For example, class UnicodeConversionMixin might provide a method unicode_to_ascii() when included in class FileReader and class WebPageScraper, which don't share a common parent.\nAbstract classes cannot be instantiated into objects; they exist only for the purpose of inheritance into other \"concrete\" classes that can be instantiated. In Java, the codice_6 keyword can be used to prevent a class from being subclassed.\nThe doctrine of composition over inheritance advocates implementing has-a relationships using composition instead of inheritance. For example, instead of inheriting from class Person, class Employee could give each Employee object an internal Person object, which it then has the opportunity to hide from external code even if class Person has many public attributes or methods. Some languages, like Go do not support inheritance at all.\nThe \"open/closed principle\" advocates that classes and functions \"should be open for extension, but closed for modification\".\nDelegation is another language feature that can be used as an alternative to inheritance.\nPolymorphism.\nSubtyping \u2013 a form of polymorphism \u2013 is when calling code can be independent of which class in the supported hierarchy it is operating on \u2013 the parent class or one of its descendants. Meanwhile, the same operation name among objects in an inheritance hierarchy may behave differently.\nFor example, objects of type Circle and Square are derived from a common class called Shape. The Draw function for each type of Shape implements what is necessary to draw itself while calling code can remain indifferent to the particular type of Shape being drawn.\nThis is another type of abstraction that simplifies code external to the class hierarchy and enables strong separation of concerns.\nOpen recursion.\nIn languages that support open recursion, object methods can call other methods on the same object (including themselves), typically using a special variable or keyword called codice_7 or codice_8. This variable is \"late-bound\"; it allows a method defined in one class to invoke another method that is defined later, in some subclass thereof.\nOOP languages.\nSimula (1967) is generally accepted as being the first language with the primary features of an object-oriented language. It was created for making simulation programs, in which what came to be called objects were the most important information representation. Smalltalk (1972 to 1980) is another early example, and the one with which much of the theory of OOP was developed. Concerning the degree of object orientation, the following distinctions can be made:\nOOP in dynamic languages.\nIn recent years, object-oriented programming has become especially popular in dynamic programming languages. Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.\nThe Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript/ECMAScript language. JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming). Another scripting language that takes this approach is Lua.\nOOP in a network protocol.\nThe messages that flow between computers to request services in a client-server environment can be designed as the linearizations of objects defined by class objects known to both the client and the server. For example, a simple linearized object would consist of a length field, a code point identifying the class, and a data value. A more complex example would be a command consisting of the length and code point of the command and values consisting of linearized objects representing the command's parameters. Each such command must be directed by the server to an object whose class (or superclass) recognizes the command and is able to provide the requested service. Clients and servers are best modeled as complex object-oriented structures. Distributed Data Management Architecture (DDM) took this approach and used class objects to define objects at four levels of a formal hierarchy:\nThe initial version of DDM defined distributed file services. It was later extended to be the foundation of Distributed Relational Database Architecture (DRDA).\nDesign patterns.\nChallenges of object-oriented design are addressed by several approaches. Most common is known as the design patterns codified by Gamma \"et al.\". More broadly, the term \"design patterns\" can be used to refer to any general, repeatable, solution pattern to a commonly occurring problem in software design. Some of these commonly occurring problems have implications and solutions particular to object-oriented development.\nInheritance and behavioral subtyping.\nIt is intuitive to assume that inheritance creates a semantic \"is a\" relationship, and thus to infer that objects instantiated from subclasses can always be \"safely\" used instead of those instantiated from the superclass. This intuition is unfortunately false in most OOP languages, in particular in all those that allow mutable objects. Subtype polymorphism as enforced by the type checker in OOP languages (with mutable objects) cannot guarantee behavioral subtyping in any context. Behavioral subtyping is undecidable in general, so it cannot be implemented by a program (compiler). Class or object hierarchies must be carefully designed, considering possible incorrect uses that cannot be detected syntactically. This issue is known as the Liskov substitution principle.\nGang of Four design patterns.\n\"Design Patterns: Elements of Reusable Object-Oriented Software\" is an influential book published in 1994 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the \"Gang of Four\". Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them.\nAs of April 2007, the book was in its 36th printing.\nThe book describes the following patterns:\nObject-orientation and databases.\nBoth object-oriented programming and relational database management systems (RDBMSs) are extremely common in software . Since relational databases don't store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. There are a number of approaches to cope with this problem, but no general solution without downsides. One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails' ActiveRecord.\nThere are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.\nReal-world modeling and relationships.\nOOP can be used to associate real-world objects and processes with digital counterparts. However, not everyone agrees that OOP facilitates direct real-world mapping (see Criticism section) or that real-world mapping is even a worthy goal; Bertrand Meyer argues in \"Object-Oriented Software Construction\" that a program is not a model of the world but a model of some part of the world; \"Reality is a cousin twice removed\". At the same time, some principal limitations of OOP have been noted.\nFor example, the circle-ellipse problem is difficult to handle using OOP's concept of inheritance.\nHowever, Niklaus Wirth (who popularized the adage now known as Wirth's law: \"Software is getting slower more rapidly than hardware becomes faster\") said of OOP in his paper, \"Good Ideas through the Looking Glass\", \"This paradigm closely reflects the structure of systems 'in the real world', and it is therefore well suited to model complex systems with complex behaviours\" (contrast KISS principle).\nSteve Yegge and others noted that natural languages lack the OOP approach of strictly prioritizing \"things\" (objects/nouns) before \"actions\" (methods/verbs). This problem may cause OOP to suffer more convoluted solutions than procedural programming.\nOOP and control flow.\nOOP was developed to increase the reusability and maintainability of source code. Transparent representation of the control flow had no priority and was meant to be handled by a compiler. With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.\nResponsibility- vs. data-driven design.\nResponsibility-driven design defines classes in terms of a contract, that is, a class should be defined around a responsibility and the information that it shares. This is contrasted by Wirfs-Brock and Wilkerson with data-driven design, where classes are defined around the data-structures that must be held. The authors hold that responsibility-driven design is preferable.\nSOLID and GRASP guidelines.\nSOLID is a mnemonic invented by Michael Feathers which spells out five software engineering design principles:\nGRASP (General Responsibility Assignment Software Patterns) is another set of guidelines advocated by Craig Larman.\nCriticism.\nThe OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity, and for overemphasizing one aspect of software design and modeling (data/objects) at the expense of other important aspects (computation/algorithms).\nLuca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex. The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.\nChristopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP; however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.\nIn an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.\nAlexander Stepanov compares object orientation unfavourably to generic programming:\nPaul Graham has suggested that OOP's popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".\nLeo Brodie has suggested a connection between the standalone nature of objects and a tendency to duplicate code in violation of the don't repeat yourself principle of software development.\nSteve Yegge noted that, as opposed to functional programming:\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.\nEric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency. Raymond compares this unfavourably to the approach taken with Unix and the C programming language.\nRob Pike, a programmer involved in the creation of UTF-8 and Go, has called object-oriented programming \"the Roman numerals of computing\" and has said that OOP languages frequently shift the focus from data structures and algorithms to types. Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.\nRegarding inheritance, Bob Martin states that because they are software, related classes do not necessarily share the relationships of the things they represent.\nFormal semantics.\nObjects are the run-time entities in an object-oriented system. They may represent a person, a place, a bank account, a table of data, or any item that the program has to handle.\nThere have been several attempts at formalizing the concepts used in object-oriented programming. The following concepts and constructs have been used as interpretations of OOP concepts:\nAttempts to find a consensus definition or theory behind objects have not proven very successful (however, see Abadi & Cardelli, \"A Theory of Objects\" for formal definitions of many OOP concepts and constructs), and often diverge widely. For example, some definitions focus on mental activities, and some on program structuring. One of the simpler definitions is that OOP is the act of using \"map\" data structures or arrays that can contain functions and pointers to other maps, all with some syntactic and scoping sugar on top. Inheritance can be performed by cloning the maps (sometimes called \"prototyping\").", "categories": ["Category:All articles containing potentially dated statements", "Category:All articles needing additional references", "Category:All articles with unsourced statements", "Category:Articles containing potentially dated statements from 2006", "Category:Articles needing additional references from August 2009", "Category:Articles with BNE identifiers", "Category:Articles with BNF identifiers", "Category:Articles with BNFdata identifiers", "Category:Articles with FAST identifiers", "Category:Articles with GND identifiers"]}
